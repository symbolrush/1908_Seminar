---
title: | 
    | Seminar Programmiersysteme
    | T52: testthat - Testing in R
author: |
    | 
    | Adrian Stämpfli
    | Matr.Nr. 9529020
    | 
header-includes: \usepackage[ngerman]{babel} \usepackage{graphicx} \usepackage{float} \pagenumbering{roman} 
output:
  pdf_document:
    fig_caption: yes
    highlight: tango
    number_sections: yes
    toc: yes
date: | 
    |  
    | `r format(Sys.time(), '%B %d, %Y')`
documentclass: report
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
bibliography: bibliography.bib
preamble: |
  % Any extra latex you need in the preamble
abstract: |
  Dieser Seminarbeitrag stellt das `testthat` Package für automatisierte Testen von R packages vor. Einleitend wird eine kurze Einführung in R gegeben, sowie einige grundlegende Gedanken zum Testing allgemein, sowie zum Testing im Scientific Computing vorgestellt. Abgeschlossen wird der Beitrag von einen Gedanken zur Erweiterung von `testthat`, um weitere Konzepte des Testings in R integrieren zu können.
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

\newpage
\pagenumbering{arabic} 

# Einführung in R  
2 Seiten

## Was ist R?  
R ist eine Programmiersprache.[^11]  
[siehe @Peng2014, Abschnitt 3.1 - 3.8]  

..und [@Wickham2017, Abschnitt 1.3.2 und 1.4 - 1.6]

### Getting help and learning more  
[@Wickham2017, Abschnitt 1.6]



### RStudio  
[@Wickham2017, Abschnitt 1.4.2]

## Warum R?  
Nützlich für Scientific Computing und Data Sciences. [@Peng2014, Introduction]

[^11: https://www.r-project.org/]

\newpage

# Testing in R  

## Warum Testing?  
Testing ist essenziell, denn jede Software wird getestet. [@Hunt2000] schreiben dazu in Ihrem Buch "The Pragmatic Programmer": "Test Your Software, or Your Users Will". Also lieber selber die Bugs finden, als von seinen Kunden (oder Kollegen) darauf hingewiesen werden.  
Dennoch testen viele Programmierer nicht gerne. Testen wird als notwendiges Übel betrachtet, als Zusatzaufwand den es zu minimieren gilt. Häufig wird daher "nett" getestet: Kritische Stellen werden eher umschifft und nicht auf Herz und Nieren geprüft. [@Hunt2000, 237ff.]  
Im Scientific Computing ist die Situation dabei eher noch schlechter als in der traditionellen Softwareentwicklung. Dies hat mehrere Gründe. [@Wilson2014] schreiben, dass Wissenschaftler zwar mehr und mehr Zeit mit der Entwicklung von Software verbringen, jedoch meistens kaum fundierte Kenntnisse in Softwareentwicklung haben. Dabei ist Software für die heutige Wissenschaft so wichtig wie Reagenzgläser und Teleskope. In der Folge wird von denselben Wissenschaftler, welche teure Messgeräte einsetzen und diese regelmässig kalibrieren schlecht getesteter Code für die Auswertung der Messdten benutzt. Die Folge sind nicht selten falsche Ergebnisse. [@Wilson2014] geben eine ganze Reihe von Artikeln aus bekannten wissenschaftlichen Journals an, welche sich mit der Richtigstellung von falschen Untersuchungen aufgrund von Programmierfehlern beschäftigen. Testing ist also wichtig, wenn die Software welche getestet wird nicht von zahlenden Kunden genutzt wird.  
[@Hunt2000]
Tools, welche das (automatisierte) Testen unterstützen gibt es viele. Alleine in R gibt es mit `svUnit` und `rUnit` zwei Implementierungen des XUnit Frameworks.  
Warum also braucht es mit `testthat` nochmals ein neues Framework. [@Hunt2000] argumentieren in Ihrem Buch, dass Testing mehr ein kulturelles als ein technisches Problem sei. Die Kultur des Testens könne in ein Projekt eingebunden werden unabhängig von der Programmiersprache. [@Hunt2000, S.197]  
Genau hier setzt `testthat` an. Der Entwickler von Testthat Hadley Wickham, Chief Scientist bei RStudio und Autor mehrerer Bücher und der wichtigsten R packages der letzten Jahre [^21], schreibt hierzu, dass Software Testing wichtig sei, aber, dass es viele nicht tun, weil es frustrierend und langweilig ist. [@Wickham2011]  `testthat` versucht dies zu ändern, indem es sich besonders einfach in den Workflow von R Programmierern einbinden lässt, einfach zu lernen ist und gut skaliert. Insbesondere der schnell mögliche Einstieg in `testthat` versucht genau die "Kultur des Testens" zu katalysieren.  
Er argumentiert, dass automatisiertes Testen zwar etwas Zusatzaufwand bedeutet, sich jedoch in viererlei Hinsichten ausbezahlt:  

1. **Weniger Frustration**. Die Arbeit an der aktuell zu entwickelnden Software aufzugeben, um Bugs in (alter) Software zu suchen ist frustrierend. Gut getestete Software hat weniger Bugs, weniger Zeit geht mit der mühsamen Suche nach (alten) Bugs verloren.  
2. **Bessere Code Struktur**. Gut geschriebener Code ist einfacher zu testen. Häufig führt eine Kultur des Testens auch dazu, dass der Code umgeschrieben wird, bis er besser testbar ist. Dasselbe Argument führen auch [@Hunt2000] ins Feld.  
3. **Besserer Wiedereinstieg in die Arbeit nach einer (längeren) Pause**. Wenn eine Codeing-Sitzung damit abgeschlossen wird einen Test für das nächste zu entwickelnde Feature zu schreiben ist bei der Wiederaufnahme der Arbeit sofort klar, welcher Test einen Fehler wirft und welche Methode demzufolge als nächstes entwickelt werden sollte.  
4. **Mehr Vertrauen bei Änderungen am Code**. Wenn ein Entwickler weiss, dass jede Funktion gut getestet ist, lassen sich Änderungen mit viel besserem Gefühl umsetzen.  

Meine Erfahrung bei der Entwicklung von `sim911`, einer Sammlung von R packages für die Analyse und Simulation von Rettungsdiensteinsatzdaten, welche ich 2014 begonnen habe und in der Zwischenzeit von einem 4-köpfigen Team am IMS-FHS weiterentwickelt wird hat mich zusätzlich noch einen weiteren Punkt gelernt:  
5. **Insgesamt mehr Vertrauen in den (eigenen) Code**. Einige Teile von `sim911`[^22] sind viel besser getestet als andere. Das Vertrauen in diese Teile ist viel höher, als in die anderen. Wenn ich einem Kollegen helfe einen Bug in einem Projekt zu finden ist dieses Vertrauen essentiell. Bei den schlecht getesteten Teilen bin ich mir einige Jahre nach dem Entwickeln selber nicht mehr sicher, ob sie jetzt mit anderen Situationen umgehen können oder nicht. Dies führt dazu, dass immer mal wieder Zeit damit verloren geht, das Vertrauen in (alten) Code zurückzugewinnen. Häufig ist der Bug dann doch nicht im schlecht getesteten Code. Wäre der Code gut getestet, wäre dies jedoch von Beginn weg klar.     

[@Wickham2015] noch einarbeiten..

[^21: https://www.tidyverse.org/]
[^22: https://www.fhsg.ch/fhs.nsf/files/IMS_Rettungswesen_sim911Bericht/$FILE/1%20-%20sim911%20-%20Ein%20Simulator%20fu%CC%88r%20das%20Rettungswesen.pdf]

[@Wilson2014, Abschnitt "Plan for Mistake" und Einführung]
[@Hunt2000, 189ff., 237ff.]

## Testing Approaches in R  
In R existieren viele Testing Approaches. [@YihuiXie], Autor von `knitr` und weiteren R packages im Bereich "Publishing", nennt in seinem Blogbeitrag "Testing R Packages" drei formale Approaches und schlägt mit seinem package `testit` noch einen vierten vor. Zusätzlich ist sicherlich noch das informelle Testen von R Code in der Konsole zu nennen, laut [@Wickham2011] eine häufige Praxis in der R Community.  

Die 5 Testing Approaches sind demnach:  

1. Testing in der Konsole  
2. Testing mittels Textvergleich  
3. `RUnit` und `svUnit`  
4. `testit`  
5. `testthat`    


### Testing in der Konsole  
R ist eine Skriptsprache. Das heisst jede Zeile Code kann zu jedem Zeitpunkt ohne Compilieren ausgeführt werden. R Programmierer nutzen diese Eigenschaft typischerweise sehr intensiv: Bei der Entwicklung einer Funktion werden die einzelnen Bestandteile direkt in der Konsole getestet, korrigiert und danach im Skript platziert. Das heisst das Testing in der Konsole ist typischerweise die erste Variante, wie R Code getestet wird. [@Wickham2011] schreibt hierzu: "es ist nicht so, dass wir unseren Code nicht testen, aber wir speichern die Tests nicht, so dass wir sie automatisert wieder laufen lassen können."

### Testing mittels Textvergleichs  
Testing mittels Textvergleich ist ein älteres Testverfahren, welches in der R Community weit verbreitet ist. Die R Core packages beispielsweise werden auf diese Weise getestet.  
Dabei werden Testfälle unter `package/tests/` gespeichert. Mittels `R CMD BATCH foo-test.R` werden die Tests ausgeführt. Die Outputs aus dem Test werden dadurch im File `foo-test.Rout.save` gespeichert. Das Testing geschieht danach indem das File `foo-test.Rout`, generiert mit `R CMD check` mit dem vorab gespeicherten `foo-test.Rout.save` verglichen wird. Die Funktion `R CMD check` informiert den Tester automatisch, falls Differenzen bestehen. [@YihuiXie]

### `RUnit` und `svUnit`   
Mit `RUnit` und `svUnit` existieren zwei packages, welche das XUnit Framework in R implementieren. `RUnit` und `svUnit` sind ähnlich aufgebaut und orientieren sich an `jUnit`. Beide packages konnten sich in der R Community jedoch nicht durchsetzen. [@Wickham2011] und [@YihuiXie] schreiben, dass der Einarbeitungs- und Einrichtungsaufwand wohl bei beiden Packages zu gross sei.

### Testing mittels `testit`  
For me, I only want one thing for unit testing: I want the non-exported functions to be visible to me during testing; unit testing should have all “units” available, but R’s namespace has intentionally restricted the objects that are visible to the end users of a package, which is a Very Good Thing to end users. It is less convenient to the package author, since he/she will have to use the triple colon syntax such as foo:::hidden_fun() when testing the function hidden_fun().

I did not like the first approach (text comparison), and I did not want to learn or remember the new vocabulary of RUnit or testthat. There is only one function for the testing purpose in this package: assert().



### Testing mittels `testthat`  
`testthat` ist neuer als `RUnit` und `svUnit`. Der Autor Hadley Wickham ist einer der wichtigsten R-Entwickler der letzten Jahre, wenn nicht der wichtigste überhaupt.[^23]  

`testthat` hat sich in den letzten Jahren als de-facto Standard für das automatisierte Unit-testing in R etabliert.[^24] Im Folgenden wollen wir uns daher vertiefter mit `testthat` auseinandersetzen.


tests are expressed as expect_something() like a natural human language


[^23]: Laut https://www.rdocumentation.org ist Hadley Wickham der Topautor überhaupt. Seine packages werden durch die Community am meisten genutzt.
[^24]: Die Zunahme an packages mit formalen Tests ist quasi alleine auf testthat zurückzuführen: https://github.com/rstudio/webinars/blob/master/28-covr/covr-Rstudio_webinar.pdf


# `testthat`  
[@Wickham2015], [@Wickham2011]


## Test Workflow   
Die Integration von `testthat` in ein bestehendes Package ist enorm einfach. Die Funktion `devtools::use_testthat` aus dem `devtools`[^31] Package erstellt die nötigen Ordner und Files. Dies sind:

1. Ein Ordner `tests/testthat` im Verzeichnis des aktuellen Packages.  
2. Ein File `tests/testthat.R`, welches alle Tests ausführt, wenn `R CMD check` aufgerufen wird. Dies ist wichtig für das Deployment eines Packages auf CRAN.  

Auch der weitere Workflow ist erdenklich einfach:  

1. Änderungen an Code oder Tests vornehmen.  
2. Das Package testen mit `devtools::test()` oder Ctrl/Cmd-Shift-T.  
3. Schritte 1 und 2 wiederholen, bis das Package fehlerfrei ist.  

`testthat` erstellt während dem Testen automatisch einen Statusreport in der Konsole oder in RStudio.

## Test Struktur   
`testthat` organisiert die Tests in einer hierarchischen Struktur. *expectations* werden zu *tests* zusammengefasst, mehrere *tests* werden in einem *context* zusammengefasst. Typischerweise besteht ein 1:1 Matching zwischen *context* und Datei. Dies ist aber nicht eine Notwendigkeit, eine Datei kann auch mehrere *contexts* enthalten.  

- *expectations*, also Erwartungen, sind die kleinste Einheit des Testens. Mit einer *expectation* wird das bei einer Berechnung erwartete Resultat beschrieben.  
- Ein *test* gruppiert mehrere *expectations*, um eine Eigenschaft einer Funktion zu testen. Im Falle von einfachen Funktionen kann ein *test* auch eine gesamte Funktion testen.  
- Der *context* gruppiert mehrere zusammengehörige Tests und gibt der Gruppe von Tests einen verständlichen Namen, welcher auch im Statusreport erscheint.  

## Expectations - Erwartungen  
Expectations sind die kleinste Einheit des Testens. Eine Expectation macht eine binäre Assertion darüber, ob das Resultat eines Funktionsaufrufs mit dem erwarteten Resultat übereinstimmt. Alle Expectations haben dieselbe Struktur:  

1. Die Funktionensnamen starten mit `expect_`
1. Sie haben zwei Argumente, das erste Argument ist das Resultat der zu testenden Funktion, das zweite das erwartete Resultat.
1. Eine Expectation wirft einen Fehler, wenn die beiden Resultate nicht übereinstimmen.

Das `testthat` Package umfasst mittlerweile an die 30 Expectations. Die wichtigsten daraus werden im Folgenden einzeln vorgestellt.  

> Eventuell umschreiben, anhand Dokumentation von testthat. Die Organisation dort ist: Test auf Gleichheit, Vergleichstests, etc...
https://cran.r-project.org/web/packages/testthat/testthat.pdf

### `expect_equal()` und `expect_identical()`  
afdad

### `expect_match()`  


### `expect_message()`, `expect_warning()` und `expect_error`  

### `expect_is()`

### Weitere Expectations  



## Tests  
Jeder Test sollte erstens einen informativen Namen haben und zweitens eine einzige Funktionalität testen. Der informative Namen beschreibt dabei die Idee des Tests. [@Wickham2015] schlägt vor, dass der Name des Tests den Satz "Test that..." vervollständigt. Diese Konvention erhöht die Lesbarkeit und hilft die Idee des Tests auch in Zukunft zu verstehen. Die Namensgebung entlangt dem Pattern "Test that..." ist ebenfalls eine schöne Anwendung des Axioms "Write programs for people, not computers", wie es u.A. [@Wilson2014] erwähnen. Dass ein Test nur eine einzige Funktionalität testen soll ist ebenfalls einsichtig: so ist direkt klar, welche Funktion einen Fehler verursacht. Tests werden in `testthat` mithilfe der Funktion `test_that()` geschrieben. `test_that()` nimmt zwei Argumente, das Argument `desc` enthält den Namen des Tests und das Argument `code` die Testanweisungen in geschleiften Klammern. Das folgende Beispiel zeigt diese Teststruktur. Das Beispiel stammt aus der Dokumentation von `testthat`.[^32]

```{r, eval = FALSE}
test_that("trigonometric functions match identities", {
  expect_equal(sin(pi / 4), 1 / sqrt(2))
  expect_equal(cos(pi / 4), 1 / sqrt(2))
  expect_equal(tan(pi / 4), 1)
})
```

Die Zuordnung der Expectations zu Tests ist in `testthat` nicht eingeschränkt. Ein Test kann 1:n Expectations enthalten. [@Wickham2015] schlägt vor, nicht zu viele Expectations einem Test zuzuordnen, so dass die Fehlersuche schnell vonstatten geht.

Jeder Test wird von R in einem eigenen Environment ausgeführt und ist in sich abgeschlossen. Alle zur Durchführung eines Tests notwendigen Daten müssen also in diesem Test generiert werden. Das folgende Beispiel zeigt diesen Sachverhalt:

```{r, eval = FALSE}
test_that("numbers are added correctly", {
  a <- 2
  b <- 1
  expect_equal(a + b, 3)
})

test_that("the result of an addition is still a numeric", {
  expect_is(a + b, "numeric")
})
```

Der erste Test überprüft, dass die Addition `a + b` das richtige Resultat zurückgibt, der zeite Test prüft die Klasse des Resultats. Dieser Code kann so jedoch nicht ausgeführt werden, da der zweite Test die beiden Variablen `a` und `b` nicht kennt, gibt R einen Fehler

```{r, eval = FALSE}
Fehler: Test failed: 'the result of an addition is still a numeric'
* Objekt 'a' nicht gefunden
```

zurück. Werden die beiden Variablen jedoch ausserhalb der Tests definiert, findet R sie im `Global Environment` und die beiden Tests laufen korrekt durch.  

```{r, eval = FALSE}
a <- 2
b <- 1

test_that("numbers are added correctly", {
  expect_equal(a + b, 3)
})

test_that("the result of an addition is still a numeric", {
  expect_is(a + b, "numeric")
})
```

`testthat` detektiert jedoch nicht, ob ein Test etwas an der R "Landschaft" ändert. Änderungen am Dateisystem, am Suchpfad (aufgrund von Calll der Funktionen `library()` oder `attach()`) und an den globalen Optionen (`options()` und `par()`) werden nicht erkannt.  

Wenn Tests diese Aktivitäten nutzen, muss der Anwender selber sicherstellen, dass keine unerwünschten Nebenwirkungen auftreten. Obwohl viele andere Testing Frameworks dies sicherstellen argumentiert [@Wickham2015], dass dies im Kontext von `testthat` nicht so wichtig sei. Wie das Beispiel oben zeigt ist es einerseits möglich Objekte, welche von mehreren Tests genutzt werden ausserhalb der Tests zu definieren. Für das Rückgängigmachen anderer Aktionen schlägt [@Wickham2015] die Nutzung regulärer R Funktionalität vor.

[^32]: https://www.rdocumentation.org/packages/testthat/versions/1.0.2/topics/test_that

## Kontexte und Dateien  
Der Kontext ist die oberste Organisationsstruktur innerhalb von `testthat`. Ein Kontext gruppiert eine Menge von Tests, welche verwandte Funktionalität testen. Ein Kontext wird durch den Call der Funktion `context("Name des Kontextes")` etwabliert. Die Namen der Kontexte erscheinen auch im Konsolenoutput beim Testen. 

```{r, eval = FALSE}
context("Adding numbers with R")

a <- 2
b <- 1

test_that("numbers are added correctly", {
  expect_equal(a + b, 3)
})

test_that("the result of an addition is still a numeric", {
  expect_is(a + b, "numeric")
})
```


Hadley Wickham schlägt in [@Wickham2015] vor, dass es eine 1:1 Zuordnung Kontext <> Datei gibt. In früheren Dokumenten [@Wickham2011] sowie in der Dokumentation der `context` Funktion ist diese Leitlinie noch weniger fix formuliert.

Aufgrund meiner eigenen Erfahrungen schlage ich den folgenden "Algorithmus" für die Organisation von Kontexten und Dateien vor:

1. Alle Tests sind in einer Datei.
1. Kontexte werden nach und nach eingeführt, sobald sich Inhaltliche Gruppierungen ergeben.
1. Sobald der Testcode einer Datei mehr als 100 Zeilen umfasst wird der längste Kontext dieser Datei in eine eigene Datei ausgelagert.

Dieses Vorgehen hat nach meinen Erfahrungen einen entscheidenden Vorteil gegenüber der von Beginn weg starren 1:1 Zuordnung. Zu Beginn einer Entwicklung ist das Naming der Funktionen und damit auch das des Testcodes (Kontexte und Tests) noch sehr volatil. Wenn hier von Beginn weg eine (zu) granulare Organisationsstruktur eingehalten wird, führt das Entweder zu häufigem Wechseln von Dateinamen und Kontexten, was ein unnötiger Aufwand ist, oder aber zu Beginn eingeführte (und häufig nicht optimale) Namensgebungen werden beibehalten, um Änderungen zu vermeiden. Die Nutzung der flexiblen Regeln nach obigem Algorithmus verhindert dies.

## Was testen?  


## Tests skippen  


## Schreiben eigener Expectations  


## Tricks für CRAN  

## `testthat` in Skripts nutzen  


```{r}
library(testthat)

add <- function(x, y) {
  return(x + y)
}

test_that("add works", {
  expect_equal(add(2, 2), 4)
  expect_error(add("2", 2))
})
```






\newpage

# Nützliche Erweiterungen zu `testthat`    
`testthat` kann sinnvoll erweitert werden.  

## `devtools`  


## `assertthat`  
[@Wilson2014, Plan for mistake], 

## `covr`  
[@Hester2017]


\newpage

# Zusammenfassung des Vorgetragenen und Bewertung der Ergebnisse

\newpage

\listoffigures

# Literatur





