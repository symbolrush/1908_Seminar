---
title: | 
    | Seminar Programmiersysteme
    | T52: testthat - Testing in R
author: |
    | 
    | Adrian Stämpfli
    | Matr.Nr. 9529020
    | 
header-includes: \usepackage[ngerman]{babel} \usepackage{graphicx} \usepackage{float} \pagenumbering{roman} 
output:
  pdf_document:
    fig_caption: yes
    highlight: tango
    number_sections: yes
    toc: yes
date: | 
    |  
    | `r format(Sys.time(), '%B %d, %Y')`
documentclass: report
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
bibliography: bibliography.bib
preamble: |
  % Any extra latex you need in the preamble
abstract: |
  Dieser Seminarbeitrag stellt das `testthat` Package für automatisiertes Testen von R packages vor. Einleitend wird eine kurze Einführung in R gegeben (Kapitel 1) und einige grundlegende Aspekte des Testing allgemein und im Kontext der Programmiersprache R vorgestellt (Kapitel 3). Im Hauptteil (Kapitel 3) wird das `testthat` Package und die Arbeit damit beschrieben. Mögliche Erweiterungen zur Integration weiterer Konzepte des Testings (Kapitel 4) und eine Zusammenfassung und Bewertung (Kapitel 5) runden den Seminarbeitrag ab.
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
library(testthat)
```

\newpage
\pagenumbering{arabic} 

# Einführung in R  


## Was ist R?  
R ist eine Programmiersprache.[^11]  
[siehe @Peng2014, Abschnitt 3.1 - 3.8]  

..und [@Wickham2017, Abschnitt 1.3.2 und 1.4 - 1.6]

### Getting help and learning more  
[@Wickham2017, Abschnitt 1.6]



### RStudio  
[@Wickham2017, Abschnitt 1.4.2]

## Warum R?  
Nützlich für Scientific Computing und Data Sciences. [@Peng2014, Introduction]

[^11: https://www.r-project.org/]

\newpage

# Testing in R  

## Warum Testing?  
Testing ist essenziell, denn jede Software wird getestet. [@Hunt2000] schreiben dazu in Ihrem Buch "The Pragmatic Programmer": "Test Your Software, or Your Users Will". Also lieber selber die Bugs finden, als von seinen Kunden (oder Kollegen) darauf hingewiesen werden.  
Dennoch testen viele Programmierer nicht gerne. Testen wird als notwendiges Übel betrachtet, als Zusatzaufwand den es zu minimieren gilt. Häufig wird daher "behutsam" getestet: Kritische Stellen werden eher umschifft und nicht auf Herz und Nieren geprüft. [@Hunt2000, 237ff.]  
Im Scientific Computing ist die Situation dabei eher noch schlechter als in der traditionellen Softwareentwicklung. Dies hat mehrere Gründe. [@Wilson2014] schreiben, dass Wissenschaftler zwar mehr und mehr Zeit mit der Entwicklung von Software verbringen, jedoch meistens kaum fundierte Kenntnisse in Softwareentwicklung haben. Dabei ist Software für die heutige Wissenschaft so wichtig wie Reagenzgläser und Teleskope. In der Folge wird von denselben Wissenschaftler, welche teure Messgeräte einsetzen und diese regelmässig kalibrieren, schlecht getesteter Code für die Auswertung der Messdaten benutzt. Die Folge sind nicht selten falsche Ergebnisse. [@Wilson2014] geben eine ganze Reihe von Artikeln aus bekannten wissenschaftlichen Journals an, welche sich mit der Richtigstellung von falschen Untersuchungen aufgrund von Programmierfehlern beschäftigen. Testing ist also auch wichtig, wenn die Software welche getestet wird nicht von zahlenden Kunden genutzt wird.  

Tools, welche das (automatisierte) Testen unterstützen gibt es seit Jahren viele. Alleine in R gibt es mit `svUnit` und `rUnit` zwei Implementierungen des XUnit Frameworks.  
Warum also braucht es mit `testthat` nochmals ein neues Framework?  

[@Hunt2000] argumentieren in Ihrem Buch "The Pragmatic Programmer", dass Testing primär ein kulturelles Problem sei und weniger ein technisches. Die Kultur des Testens könne in ein Projekt jedoch eingebunden werden unabhängig von der Programmiersprache. [@Hunt2000, S.197]  
Genau hier setzt `testthat` an. Hadley Wickham, der Entwickler von Testthat, Chief Scientist bei RStudio, Autor mehrerer Bücher und der einflussreichsten R packages der letzten Jahre [^21][^25] , schreibt hierzu, dass Software Testing wichtig sei, aber, dass es viele nicht tun, weil es frustrierend und langweilig sei. [@Wickham2011]  

`testthat` versucht dies zu ändern, indem es sich besonders einfach in den Workflow von R Programmierern einbinden lässt, einfach zu lernen ist und gut skaliert. Insbesondere der schnell mögliche Einstieg in `testthat` versucht genau die "Kultur des Testens" zu katalysieren.  
Er argumentiert, dass automatisiertes Testen zwar etwas Zusatzaufwand bedeutet, sich jedoch in viererlei Hinsichten ausbezahlt [@Wickham2011] und [@Wickham2015]:  

1. **Weniger Bugs**. Das Schreiben von Tests führt dazu, dass das Verhalten (und damit die Erwartungen) der Software an zwei Orten beschrieben sind. Dabei testen die Tests den Code und umgekehrt. Dies führt zu Software mit weniger Bugs.
2. **Bessere Code Struktur**. Gut geschriebener Code ist einfacher zu testen. Häufig führt eine Kultur des Testens auch dazu, dass der Code umgeschrieben wird, bis er besser testbar ist. Dasselbe Argument nennen auch [@Hunt2000].
3. **Besserer Wiedereinstieg in die Arbeit nach einer (längeren) Pause**. Wenn eine Codeing-Sitzung damit abgeschlossen wird, einen Test für das nächste zu entwickelnde Feature zu schreiben ist bei der Wiederaufnahme der Arbeit sofort klar, welcher Test einen Fehler wirft und welche Methode demzufolge als nächstes entwickelt werden sollte.
4. **Mehr Vertrauen bei Änderungen am Code**. Wenn ein Entwickler weiss, dass jede Funktion gut getestet ist, lassen sich Änderungen mit viel besserem Gefühl umsetzen.

Meine Erfahrung bei der Entwicklung von `sim911`[^22], einer Sammlung von R packages für die Analyse und Simulation von Rettungsdiensteinsatzdaten, welche ich 2014 begonnen habe und in der Zwischenzeit von einem 4-köpfigen Team am IMS-FHS weiterentwickelt wird, hat mich zusätzlich noch einen weiteren Punkt gelernt:  

5. **Insgesamt mehr Vertrauen in den (eigenen) Code**. Einige Teile von `sim911` sind viel besser getestet als andere. Das Vertrauen in diese Teile ist viel höher, als in die anderen. Wenn wir einen Bug in einem Projekt suchen ist dieses Vertrauen essentiell. Bei den schlecht getesteten Teilen bin ich mir einige Jahre nach dem Entwickeln selber nicht mehr sicher, ob sie jetzt mit anderen Situationen umgehen können oder nicht. Dies führt dazu, dass immer mal wieder Zeit damit verloren geht, das Vertrauen in (alten) Code zurückzugewinnen. Diese Zeit und Arbeit könnte gespart werden, unabhängig davon, ob der Bug schlussendlich im schlecht getesteten (alten) Code oder im aktuell entwickelten Code steckt.     


[^21]: https://www.tidyverse.org/
[^22]: https://www.fhsg.ch/fhs.nsf/files/IMS_Rettungswesen_sim911Bericht/$FILE/1%20-%20sim911%20-%20Ein%20Simulator%20fu%CC%88r%20das%20Rettungswesen.pdf  
[^25]: Laut https://www.rdocumentation.org/ ist Hadley Wickham der einflussreichste Autor von R packages. Gemessen wird der Einfluss an der Verbreitung der packages in der Community (über Downloads) und der Dependencies anderer packages auf die packages des Autors.


## Testing Approaches in R  
In R existieren viele Testing Approaches. Yihui Xie, Autor von `knitr` und weiteren R packages im Bereich "Publishing", nennt in seinem Blogbeitrag "Testing R Packages" [@YihuiXie] drei formale Approaches und schlägt mit seinem package `testit` noch einen vierten vor. Zusätzlich ist sicherlich noch das informelle Testen von R Code in der Konsole zu nennen, laut Hadley Wickham und auch aus eigener Erfahrung eine häufige Praxis in der R Community [@Wickham2011].  

Die 5 Testing Approaches sind demnach:  

1. Testing in der Konsole
2. Testing mittels Textvergleich
3. `RUnit` und `svUnit`
4. `testit`
5. `testthat`


### Testing in der Konsole  
R ist eine Skriptsprache. Das heisst jede Zeile Code kann zu jedem Zeitpunkt ohne Compilieren ausgeführt werden. R Programmierer nutzen diese Eigenschaft typischerweise sehr intensiv: Bei der Entwicklung einer Funktion werden die einzelnen Bestandteile direkt in der Konsole getestet, korrigiert und danach im Skript platziert. Das heisst das Testing in der Konsole ist typischerweise die erste Variante, wie R Code getestet wird. [@Wickham2011] schreibt hierzu: "es ist nicht so, dass wir unseren Code nicht testen, aber wir speichern die Tests nicht, so dass wir sie automatisert wieder laufen lassen können."

### Testing mittels Textvergleichs  
Testing mittels Textvergleich ist ein älteres Testverfahren, welches in der R Community weit verbreitet ist. Die R Core packages beispielsweise werden auf diese Weise getestet.  
Dabei werden Testfälle unter `package/tests/` gespeichert. Mittels `R CMD BATCH foo-test.R` werden die Tests ausgeführt. Die Outputs aus dem Test werden dadurch im File `foo-test.Rout.save` gespeichert. Das Testing geschieht danach indem das File `foo-test.Rout`, generiert mit `R CMD check` mit dem vorab gespeicherten `foo-test.Rout.save` verglichen wird. Die Funktion `R CMD check` informiert den Tester automatisch, falls Differenzen bestehen. [@YihuiXie]

### `RUnit` und `svUnit`   
Mit `RUnit` und `svUnit` existieren zwei packages, welche das XUnit Framework in R implementieren. `RUnit` und `svUnit` sind ähnlich aufgebaut und orientieren sich an `jUnit`. Beide packages konnten sich in der R Community jedoch nicht durchsetzen. [@Wickham2011] und [@YihuiXie] schreiben, dass der Einarbeitungs- und Einrichtungsaufwand wohl bei beiden Packages zu gross sei.

### Testing mittels `testit`  
`testit` ist ein kleines Paket, welches ebenfalls automatisiertes Testen ermöglicht, jedoch auf ein absolutes Minimum beschränkt ist. Die Absicht von `testit` ist einzig die, die nicht exportierten Funktionen während dem Testing zur Verfügung zu haben. Der Autor Yihui Xie schreibt dazu in [@YihuiXie]: 

> "For me, I only want one thing for unit testing: I want the non-exported functions to be visible to me during testing; unit testing should have all “units” available, but R’s namespace has intentionally restricted the objects that are visible to the end users of a package, which is a Very Good Thing to end users. It is less convenient to the package author, since he/she will have to use the triple colon syntax such as foo:::hidden_fun() when testing the function hidden_fun().

Weiter schreibt er, dass er den Ansatz des Textvergleichs nicht mag und er kein neues Vokablular lernen wollte, wie es die formaleren Frameworks wie `testthat` und `RUnit` erfordern. Das gesamte Testen im `testit` Packgage geschieht dann auch mit einer einzigen Funktion `assert()`. Das sehr einfache `testit` Package bedeutet wohl (noch) weniger Aufwand, als das Testen mit dem `testthat()` Package. Das `testthat` Package stellt jedoch sehr viele nützliche Funktionalität zur Verfügung und wir wollen uns im folgenden daher vertieft mit dem `testthat` Package beschäftigen.


### Testing mittels `testthat`  
`testthat` ist neuer als `RUnit` und `svUnit`. Der Autor Hadley Wickham ist einer der wichtigsten R-Entwickler der letzten Jahre, wenn nicht der wichtigste überhaupt.[^23]  

`testthat` hat sich in den letzten Jahren als de-facto Standard für das automatisierte Unit-testing in R etabliert.[^24] Im Folgenden wollen wir uns daher vertiefter mit `testthat` auseinandersetzen.


Tests werden im `testthat` Package mit der Syntax `expect_something()` beschrieben. Diese Art der Erwartungsbeschreibung ist an die natürliche menschliche Sprache angelehnt und erlaubt es sehr einfach Erwartungen zu formulieren, welche gut lesbar und dadurch auch verständlich sind durch die Tester.


[^23]: Laut https://www.rdocumentation.org ist Hadley Wickham der Topautor überhaupt. Seine packages werden durch die Community am meisten genutzt.
[^24]: Die Zunahme an packages mit formalen Tests ist quasi alleine auf testthat zurückzuführen: https://github.com/rstudio/webinars/blob/master/28-covr/covr-Rstudio_webinar.pdf


# Testing mit dem Package `testthat`  
Die folgenden Erläuterungen zum `testthat` Package basieren im Wesentlichen auf den beiden wichtigsten Primärquellen des Autors Hadley Wickham selber. Diese sind erstens ein Paper, welches Hadley Wickham beim Erscheinen von `testthat` geschrieben hat [@Wickham2011] und zweitens das Buch "R Packages", in welchem er den Entwicklungsprozess von R Packages detailliert beschreibt und dabei auch auf das Testing mit `testthat` eingeht. [@Wickham2015]

## Test Workflow   
Die Integration von `testthat` in ein bestehendes Package ist enorm einfach. Die Funktion `devtools::use_testthat` aus dem `devtools`[^31] Package erstellt die nötigen Ordner und Files. Dies sind:

[^31]: Devtools ist ein package, welches Tools für die Entwicklung von R packages zur Verfügung stellt. Devtools wird im Kapitel 4 näher beschrieben. verschiedenehttps://cran.r-project.org/web/packages/devtools/devtools.pdf

1. Ein Ordner `tests/testthat` im Verzeichnis des aktuellen Packages.
1. Ein File `tests/testthat.R`, welches alle Tests ausführt, wenn `R CMD check` aufgerufen wird. Dies ist wichtig für das Deployment eines Packages auf CRAN.

Auch der weitere Workflow ist erdenklich einfach:  

1. Änderungen an Code oder Tests vornehmen.
2. Das Package testen mit `devtools::test()` oder Ctrl/Cmd-Shift-T.
3. Schritte 1 und 2 wiederholen, bis das Package fehlerfrei ist.

`testthat` erstellt während dem Testen automatisch einen Statusreport in der Konsole oder in RStudio.

## Test Struktur   
`testthat` organisiert die Tests in einer hierarchischen Struktur. *expectations* werden zu *tests* zusammengefasst, mehrere *tests* werden in einem *context* zusammengefasst. Typischerweise besteht ein 1:1 Matching zwischen *context* und Datei. Dies ist aber nicht eine Notwendigkeit, eine Datei kann auch mehrere *contexts* enthalten.  

- *expectations*, also Erwartungen, sind die kleinste Einheit des Testens. Mit einer *expectation* wird das bei einer Berechnung erwartete Resultat beschrieben.  
- Ein *test* gruppiert mehrere *expectations*, um eine Eigenschaft einer Funktion zu testen. Im Falle von einfachen Funktionen kann ein *test* auch eine gesamte Funktion testen.  
- Der *context* gruppiert mehrere zusammengehörige Tests und gibt der Gruppe von Tests einen verständlichen Namen, welcher auch im Statusreport erscheint.  

## Expectations - Erwartungen  
Expectations sind die kleinste Einheit des Testens. Eine Expectation macht eine binäre Assertion darüber, ob das Resultat eines Funktionsaufrufs mit dem erwarteten Resultat übereinstimmt. Alle Expectations haben dieselbe Struktur:  

1. Die Funktionensnamen starten mit `expect_`
1. Sie haben zwei Argumente, das erste Argument ist das Resultat der zu testenden Funktion, das zweite das erwartete Resultat.
1. Eine Expectation wirft einen Fehler, wenn die beiden Resultate nicht übereinstimmen.

Das `testthat` Package umfasst mittlerweile an die 30 Expectations. Die Expectations lassen sich, nach der in verschiedene Kategorien, auf was getestet wird, einteilen. Diese sind:

1. Gleichheit
1. Vergleiche
1. String-Matching
1. Output
1. Vererbung
1. Logische Aussagen

### Gleichheit  
Für den Test auf Gleichheit stellt `testthat` drei Funktionen zur Verfügung. Dies sind `expect_equal()`, `expect_identical()` und `expect_equivalent()`.

`expect_identical()` nutzt die R base Funktion `identical()` und testet, ob zwei Objekte exakt gleich sind.  
`expect_equal()` und `expect_equivalent()` nutzen dagegen die R base Funktion `all.equal()`, welche innerhalb einer nummerischen Toleranz testet. Das folgende Beispiel verdeutlicht diesen Sachverhalt:

```{r, error = TRUE}
a <- 1
b <- 1 + 1e-8

test_that("Gleichheitstests 1", {
  expect_equal(a, b)
  expect_identical(a, b)
})
```

`expect_equal()` gibt keinen Fehler zurück, `expect_identical()` wirft einen Fehler.

`expect_equivalent()` nutzt ebenfalls `all.equal()`, setzt aber zustäzlich die Option `check.attributes = FALSE`, wodurch Differenzen in den Attributen der verglichenen Objekte nicht betrachtet werden, sondern nur die konkrete Wertebelegung. Das folgende Beispiel verdeutlicht diesen Sachverhalt:

```{r, error = TRUE}
a <- data.frame(a = 1)
b <- data.frame(b = 1 + 1e-8)

test_that("Gleichheitstests 2", {
  expect_equivalent(a, b)
  expect_equal(a, b)
  expect_identical(a, b)
})
```

Einzig `expect_equivalent()` gibt keinen Fehler zurück, da die Kolonnennamen innerhalb des Dataframes wegen der Option `check.attributes = FALSE` nicht betrachtet werden.

### Vergleiche  
`testthat` stellt 4 Funktionen zur Verfügung für Vergleichstests.

* `expect_lt()`: Testet, ob ein Objekt strikt kleiner als ein Vergleichswert ist.
* `expect_lte()`: Testet, ob ein Objekt kleiner oder gleich einem Vergleichswert ist.
* `expect_gt()`: Testet, ob ein Objekt strikt grösser als ein Vergleichswert ist.
* `expect_gte()`: Testet, ob ein Objekt grösser oder gleich einem Vergleichswert ist.



### String-Matching  
`expect_match()`, testet mithilfe der R base Funktion `grepl`, ob ein Objekt mit einer Regular-Expression matcht.  

### Output  
Die Expectations `expect_output()`, `expect_message()`, `expect_warning()` und `expect_error` sind Spezialisierungen von `expect_match()` und und vergleichen den Output eines Befehls mit einer Regular-Expression.

Das folgende Beispiel zeigt die Differenz zwischen `expect_match()` und `expect_output()`.

```{r, error = TRUE}
test_that("Unterschied zwischen `exepct_match()` und `expect_output`", {
  expect_match(print(2), "2")
  expect_output(print(2), "2")
})
```

Die Expectation `expect_match()` wirft im obigen Beispiel einen Fehler, da die Anweisung `print(2)` kein Stringt ist und demzufolge von `expect_match()` nicht verarbeitet werden kann. `expect_output()` hingegen wertet `print(2)` zuerst aus und vergleicht den Outputwert in der Konsole mit dem String "2", was übereinstimmt.

`expect_message()`, `expect_warning()` und `expect_error` wiederum sind Spezialisierungen von `expect_output()` und testen, ob Resultate einer Anweisung eine Nachricht, Warnung oder einen Fehler werfen, und ob diese Nachricht, Warnung oder Fehler mit einer Regular-Expression matchen.

### Vererbung  

`testthat` stellt Expectations zur Verfügung zum Testen, ob ein Objekt von einer S3, S4 erbt oder einen R base type hat. 

* `expect_is()`: Testet, ob ein Objekt von einer spezifizierten Klasse erbt. Nutzt die R base Funktion `class()`
* `expect_s3_class()`: Testet, ob ein Objekt von einer spezifierten S3 Klasse erbt. Nutzt die R base Funktion `class()`
* `expect_s4_class()`: Testet, ob ein Objekt von einer spezifierten S4 Klasse erbt. Nutzt die R base Funktion `class()`
* `expect_type()`: Testet, ob ein Objekt ein R base Typ hat. Nutzt die R base Funktion `typeof()`.
* `expect_null()`: ist ein Spezialfall und testet auf das Objekt `NULL`.  

### Logische Aussagen  

`expect_true()` und `expect_false()` erlauben das Testing logischer Aussagen.

## Tests  
Jeder Test sollte erstens einen informativen Namen haben und zweitens eine einzige Funktionalität testen. Der informative Namen beschreibt dabei die Idee des Tests. [@Wickham2015] schlägt vor, dass der Name des Tests den Satz "Test that..." vervollständigt. Diese Konvention erhöht die Lesbarkeit und hilft die Idee des Tests auch in Zukunft zu verstehen. Die Namensgebung entlangt dem Pattern "Test that..." ist ebenfalls eine schöne Anwendung des Axioms "Write programs for people, not computers", wie es u.A. [@Wilson2014] erwähnen. Dass ein Test nur eine einzige Funktionalität testen soll ist ebenfalls einsichtig: so ist direkt klar, welche Funktion einen Fehler verursacht. Tests werden in `testthat` mithilfe der Funktion `test_that()` geschrieben. `test_that()` nimmt zwei Argumente, das Argument `desc` enthält den Namen des Tests und das Argument `code` die Testanweisungen in geschleiften Klammern. Das folgende Beispiel zeigt diese Teststruktur. Das Beispiel stammt aus der Dokumentation von `testthat`.[^32]

```{r, eval = FALSE}
test_that("trigonometric functions match identities", {
  expect_equal(sin(pi / 4), 1 / sqrt(2))
  expect_equal(cos(pi / 4), 1 / sqrt(2))
  expect_equal(tan(pi / 4), 1)
})
```

Die Zuordnung der Expectations zu Tests ist in `testthat` nicht eingeschränkt. Ein Test kann 1:n Expectations enthalten. [@Wickham2015] schlägt vor, nicht zu viele Expectations einem Test zuzuordnen, so dass die Fehlersuche schnell vonstatten geht.

Jeder Test wird von R in einem eigenen Environment ausgeführt und ist in sich abgeschlossen. Alle zur Durchführung eines Tests notwendigen Daten müssen also in diesem Test generiert werden. Das folgende Beispiel zeigt diesen Sachverhalt:

```{r, eval = FALSE}
test_that("numbers are added correctly", {
  a <- 2
  b <- 1
  expect_equal(a + b, 3)
})

test_that("the result of an addition is still a numeric", {
  expect_is(a + b, "numeric")
})
```

Der erste Test überprüft, dass die Addition `a + b` das richtige Resultat zurückgibt, der zeite Test prüft die Klasse des Resultats. Dieser Code kann so jedoch nicht ausgeführt werden, da der zweite Test die beiden Variablen `a` und `b` nicht kennt, gibt R einen Fehler

```{r, eval = FALSE}
Fehler: Test failed: 'the result of an addition is still a numeric'
* Objekt 'a' nicht gefunden
```

zurück. Werden die beiden Variablen jedoch ausserhalb der Tests definiert, findet R sie im `Global Environment` und die beiden Tests laufen korrekt durch.  

```{r, eval = FALSE}
a <- 2
b <- 1

test_that("numbers are added correctly", {
  expect_equal(a + b, 3)
})

test_that("the result of an addition is still a numeric", {
  expect_is(a + b, "numeric")
})
```

`testthat` detektiert jedoch nicht, ob ein Test etwas an der R "Landschaft" ändert. Änderungen am Dateisystem, am Suchpfad (aufgrund von Calll der Funktionen `library()` oder `attach()`) und an den globalen Optionen (`options()` und `par()`) werden nicht erkannt.  

Wenn Tests diese Aktivitäten nutzen, muss der Anwender selber sicherstellen, dass keine unerwünschten Nebenwirkungen auftreten. Obwohl viele andere Testing Frameworks dies sicherstellen argumentiert [@Wickham2015], dass dies im Kontext von `testthat` nicht so wichtig sei. Wie das Beispiel oben zeigt ist es einerseits möglich Objekte, welche von mehreren Tests genutzt werden ausserhalb der Tests zu definieren. Für das Rückgängigmachen anderer Aktionen schlägt [@Wickham2015] die Nutzung regulärer R Funktionalität vor.

[^32]: https://www.rdocumentation.org/packages/testthat/versions/1.0.2/topics/test_that

## Kontexte und Dateien  
Der Kontext ist die oberste Organisationsstruktur innerhalb von `testthat`. Ein Kontext gruppiert eine Menge von Tests, welche verwandte Funktionalität testen. Ein Kontext wird durch den Call der Funktion `context("Name des Kontextes")` etwabliert. Die Namen der Kontexte erscheinen auch im Konsolenoutput beim Testen. 

```{r, eval = FALSE}
context("Adding numbers with R")

a <- 2
b <- 1

test_that("numbers are added correctly", {
  expect_equal(a + b, 3)
})

test_that("the result of an addition is still a numeric", {
  expect_is(a + b, "numeric")
})
```


Hadley Wickham schlägt in [@Wickham2015] vor, dass es eine 1:1 Zuordnung Kontext <> Datei gibt. In früheren Dokumenten [@Wickham2011] sowie in der Dokumentation der `context` Funktion ist diese Leitlinie noch weniger fix formuliert.

Aufgrund meiner eigenen Erfahrungen schlage ich den folgenden "Algorithmus" für die Organisation von Kontexten und Dateien vor:

1. Alle Tests sind in einer Datei.
1. Kontexte werden nach und nach eingeführt, sobald sich Inhaltliche Gruppierungen ergeben.
1. Sobald der Testcode einer Datei mehr als 100 Zeilen umfasst wird der längste Kontext dieser Datei in eine eigene Datei ausgelagert.

Dieses Vorgehen hat nach meinen Erfahrungen einen entscheidenden Vorteil gegenüber der von Beginn weg starren 1:1 Zuordnung. Zu Beginn einer Entwicklung ist das Naming der Funktionen und damit auch das des Testcodes (Kontexte und Tests) noch sehr volatil. Wenn hier von Beginn weg eine (zu) granulare Organisationsstruktur eingehalten wird, führt das Entweder zu häufigem Wechseln von Dateinamen und Kontexten, was ein unnötiger Aufwand ist, oder aber zu Beginn eingeführte (und häufig nicht optimale) Namensgebungen werden beibehalten, um Änderungen zu vermeiden. Die Nutzung der flexiblen Regeln nach obigem Algorithmus verhindert dies.

## Was testen?  
Nachdem wir wissen, *wie* mit `testthat` getestet werden kann, widmen wir uns nun der viel wichtigeren und nicht so eindeutig beantwortbaren Frage *was* (mit `testthat`) getestet werden soll. Im Folgenden möchte ich mich dieser "philososphischen" Frage anhand verschiedener Zitate nähern und durch meine eigene Einschätzungen abrunden.  

Starten möchte ich mit Martin Fowler[^33], einem der Gründerväter der agilen Softwareentwicklung:

> "Whenever you are tempted to type something into a print statement or a debugger expression, write it as a test instead."

Diese Guideline ist sicherlich ein guter Startpunkt. Die Frage ist jedoch, ob man so bei (zu) vielen Testfällen landet, welche künftige Änderungen am Code erschweren.

Hadley Wickham schreibt hierzu in [@Wickham2015]:

> "There is a fine balance to writing tests. Each test that you write makes your code less likely to change inadvertently; but it also can make it harder to change your code on purpose."

Hadley Wickham schreibt in seinem Buch auch, dass es schwierig sei generell gültige Aussagen zur Menge der notwendigen Tests zu geben. Wo also beginnen?

Programmierer im Umfeld von R bedeutet nutzen beim Entwickeln häufig die Interaktivität von R aus und testen sowohl einzelne Zeilen als auch ganze Funktionen interaktiv in der Konsole. Diese Testfälle direkt in `testthat` zu implementieren ist sicherlich ein guter Startpunkt und auch, wie Abschnitt [3.10 `testthat` in Skripts nutzen](#testthat_in_skripts) gezeigt wird, ohne grossen Aufwand möglich. Diese Art von Tests zu Automatisieren ist auch eine der Hauptmotivationen, welche Hadley Wickham in [@Wickham2011] darlegt:  

> "I'd perform many interactive tests to make sure the code worked, but I never had a system for retaining these tests and running them, again and again."

Ein weiterer enorm wichtiger Punkt spiegelt das folgende Zitat aus "The Pragmatic Programmer" wieder:

> "Find Bugs Once."

Sobald ein Tester einen Bug findet, sollte ein Testfall geschrieben werden, welcher den Bug abfängt. Falls derselbe Bug ein zweites Mal auftritt sollte er von `testthat` automatisch gefunden werden und nicht wieder vom Entwickler, Tester oder Anwendern. [@Hunt2000] Denselben Rat gibt auch Hadley Wickham in [@Wickham2011] und Wilson et al. geben in [@Wilson2014] den Satz:

> "Turn bugs into test cases."

als eine der vorgeschlagenen Best Practices an.

Im Umfeld des Scientific Programming ist zudem eine andere Guideline eine gute Entscheidungshilfe, wann Testfälle geschrieben werden sollen. Tests sollen sicherstellen, dass Funktionen dem Problemverständnis des Wissenschaftlers entsprechendes Verhalten aufweisen. Hierfür werden z.B. Tests geschrieben, welche Funktionsoutput mit vereinfachenden Rechnungen, mit Daten aus Experimenten oder mit Resultaten älterer, vertrauenswürdiger Software vergleichen. [@Wilson2014]

Wilson et al. schreiben hierzu:

> "Tests check to see whether the code matches the researcher's expectations of its behavior, which depends on the researcher's understanding of the problem at hand."

Eine weitere Dimension der Frage *was* man testen sollte wird in den folgenden beiden Zitaten angeschnitten.  

> "Avoid testing simple code that you're confident will work."

Dies schreibt Hadley Wickham in [@Wickham2015] und [@Hunt2000] schreiben:

> "Most developers hate testing. They tend to test gently, subconcsciously knowing where the code will break and avoiding the weak spots."

Die Menge der Testfälle garantiert eben noch keine gute Qualität der Testfälle.

Die genannten Zitate und Dimensionen der Frage nach dem *was* erlauben die Destillation der folgenden Guidelines beim Schreiben von Tests für die Nutzung von `testthat` im Rahmen des Scientific Computing:

1. **Teste (als Wissenschaftler) deine inhaltliche Erwartungshaltung.** Was ist dein Modell der Funktion, welche du am Entwickeln bist? Gibt es (vereinfachende) Testfälle (z.B. obere- oder untere Schranken einer Berechnung), welche du ohne aufwändige Testfälle angeben kannst? Gibt es obere oder untere Extremwerte, welche getestet werden können? Welche Anwendungsfälle hast du im Kopf und würdest du auf der Konsole interaktiv testen wollen? Schreibe diese als Tests nieder. Dieser Ansatz kann gut auch mit der "test-first" Philosophie kombiniert werden.[^34]
1. **Turn bugs into testcases.** Sobald ein Bug auftaucht, schreibe einen Testfall, welcher den Bug abfängt. Korrigiere den Bug danach.
1. **Sei Rücksichtslos.** Schreibe als nächstes den Test, welcher deiner Meinung nach die grösste Chance hat nicht erfolgreich durchzulaufen. Siehe hierzu auch [@Hunt2000, S.237ff.]. Die Befolgung dieser Maxime stellt ein Stück weit auch sicher, dass nicht (zu) viele Tests geschrieben werden, welche kaum einen Impact auf die Güte der Tests haben.

Aus einer Informatiksicht lassen sich zusätzlich die folgenden Guidelines nennen:

4. **Fokussiere beim Testen auf das externe Interface.** Wird nur das Interface getestet und nicht die Implementierung, lassen sich Änderungen an der Implementierung trotz Tests durchführen. Wenn auch die Implementierung getestet wird, müssen bei Änderungen am Code auch die Tests geändert werden. [@Wickham2015]
5. **Teste jedes Verhalten in genau einem Test.** Änderungen am Code ziehen so nur Änderungen an genau einem Test nach sich. [@Wickham2015]

Ungeklärt ist nun noch *mit was* getestet werden soll. Meine eigene Arbeit hat mich in den letzten Jahren gelernt, dass im Umfeld des Scientific Computing unbedingt mit synthethischen und echten Daten getestet werden muss. Die Arbeit mit synthetischen Daten ist sehr einfach möglich. Diese lassen sich meist sehr einfach genererien. Die erwarteten Resultate lassen sich mit synthetischen Daten dabei meist kognitiv nachvollziehen. Daher sind synthetische Daten enorm wertvoll um die inhaltliche Erwartungshaltung zu testen und können sehr schnell formuliert werden. Bei Tests mit echten Daten hingegen ist es viel schwieriger (oder nahezu unmöglich) das korrekte Resultat kognitiv nachzuvollziehen. Die Verifizierung von korrekten Testresultaten ist also enorm schwierig. Dagegen haben reale Daten häufig andere Eigenschaften, als die synthetischen Testdaten. Die Nutzung von zusätzlichen Tests mit realen Daten erlauben es, zu schauen, ob eine Implementierung auch mit diesen Eigenschaften umgehen kann. Falls sich korrekte Resultate auf echten Daten nicht kognitiv bestimmen lassen können diese dennoch wertvoll sein und z.B. gegenüber oberen- oder unteren Schranken getestet oder für Regressionstests verwendet werden. Ich formuliere daher noch die letzte Guideline:

6. **Nutze echte und synthetische Daten.**



[^33]: https://martinfowler.com/
[^34]: http://www.extremeprogramming.org/rules/testfirst.html

## Tests skippen   
Es gibt Situationen, in denen es sinnvoll sein kann einen Test zu überspringen, ihn zu skippen.  
Dies kann verschiedene Gründe haben:  

1. Ein Test greift auf eine externe API zu, welche momentan offline ist.
1. Ein Test greift auf eine Datei zu, welche momentan nicht verfügbar ist. (Beispielsweise, weil sie auf einem Rechner eines anderen Netzwerks liegt)
1. Tests greifen auf andere Systeme zu, welche momentan nicht zur Verfügung stehen.

In solchen Situationen kann es nützlich sein, einen Test zu überspringen. So gibt `testthat` keine Fehlermeldung zurück, sondern vermeldet einfach, dass ein oder mehrere Tests übersprungen wurden.

Zu diesem Zweck stellt `testthat` die Funktion `skip()` zur Verfügung. Statt einem Fehler zurückzugeben schreibt `skip()` ganz einfach ein $S$ in die Konsole.

```{r, eval = FALSE}
test_that("...", {
  skip("Grund für das Skippen des Tests.")
  ...
})
```

Das Beispiel oben nutzt `skip()` auf statische Art. Um den Test wieder laufen zu lassen, muss `skip()` gelöscht, bzw. auskommentiert werden. Häufig ist es eine bessere Idee `skip()` dynamisch zu nutzen. Die Bedingung unter welcher geskippt werden soll kann programmatisch abgefangen werden. Der Test wird, sobald beispielsweise eine API wieder zur Verfügung steht automatisch wieder ausgeführt. Das folgende Beispiel aus [@Wickham2015] zeigt diese Art der Nutzung.

```{r, eval = FALSE}
check_api <- function() {
  if (not_working()) {
    skip("API not available")
  }
}

test_that("foo api returns bar when given baz", {
  check_api()
  ...
})
```


## Schreiben eigener Expectations und andere Tricks  

`testthat` kann um eigene Expectations erweitert werden. Dies ist dann interessant, wenn sich im Testing Code mehr und mehr Dupllikationen auftun. Das folgende Beispiel ist dem Buch "R Packages" [@Wickham2015] entnommen.

Anhand eines Tests zur `floor_date()` des `lubridate`-Packages werden die Vorteile eigener Expectations aufgezeigt. Die Funktion `floor_date()` rundet ein Datum auf verschiedene Einheiten ab, von der Abrundung auf die letzte Sekunde bis zur Abrundung auf den Jahresbeginn.

```{r}
library(lubridate)
test_that("floor_date works for different units", {
  base <- as.POSIXct("2009-08-03 12:01:59.23", tz = "UTC")

  expect_equal(floor_date(base, "second"), 
    as.POSIXct("2009-08-03 12:01:59", tz = "UTC"))
  expect_equal(floor_date(base, "minute"), 
    as.POSIXct("2009-08-03 12:01:00", tz = "UTC"))
  expect_equal(floor_date(base, "hour"),   
    as.POSIXct("2009-08-03 12:00:00", tz = "UTC"))
  expect_equal(floor_date(base, "day"),    
    as.POSIXct("2009-08-03 00:00:00", tz = "UTC"))
  expect_equal(floor_date(base, "week"),   
    as.POSIXct("2009-08-02 00:00:00", tz = "UTC"))
  expect_equal(floor_date(base, "month"),  
    as.POSIXct("2009-08-01 00:00:00", tz = "UTC"))
  expect_equal(floor_date(base, "year"),   
    as.POSIXct("2009-01-01 00:00:00", tz = "UTC"))
})
```

Der Code ist schwierig zu überblicken, da jede Expectation mehr als eine Zeile umfasst. Die Definition zweier helper-Funktionen `floor_base` und `as_time` entfernen die Redundanz aus dem Code. Der Code wird übersichtlich. Der Test auf Gleichheit erfolgt jedoch noch immer mit der `expect_equal` Funktion. (Was nicht per se verwerflich ist..)

```{r}
test_that("floor_date works for different units", {
  base <- as.POSIXct("2009-08-03 12:01:59.23", tz = "UTC")
  floor_base <- function(unit) floor_date(base, unit)
  as_time <- function(x) as.POSIXct(x, tz = "UTC")

  expect_equal(floor_base("second"), as_time("2009-08-03 12:01:59"))
  expect_equal(floor_base("minute"), as_time("2009-08-03 12:01:00"))
  expect_equal(floor_base("hour"),   as_time("2009-08-03 12:00:00"))
  expect_equal(floor_base("day"),    as_time("2009-08-03 00:00:00"))
  expect_equal(floor_base("week"),   as_time("2009-08-02 00:00:00"))
  expect_equal(floor_base("month"),  as_time("2009-08-01 00:00:00"))
  expect_equal(floor_base("year"),   as_time("2009-01-01 00:00:00"))
})
```

Man kann jedoch noch einen Schritt weitergehen und eine eigene Funktion `expect_floor_equal()` definieren, welche `expect_equal()` und `floor_base()` zusammenfasst.

```{r}
base <- as.POSIXct("2009-08-03 12:01:59.23", tz = "UTC")

expect_floor_equal <- function(unit, time) {
  expect_equal(floor_date(base, unit), as.POSIXct(time, tz = "UTC"))
}
expect_floor_equal("year", "2009-01-01 00:00:00")
```

Somit ist die erste eigene Expectation geschrieben. Diese Expectation gibt jedoch noch keine sinnvollen Fehlermeldungen zurück, was einen der grossen Vorteile des `testthat` Packages zunichte macht. Das folgende Beispiel zeigt dieses Problem:

```{r, error = TRUE, eval = FALSE}
test_that("Beispiel X", {
  expect_floor_equal("year", "2008-01-01 00:00:00")
})
```

Mithilfe einer anderen Auswertungsart kann dieses Problem umgangen werden. [non-standard evaluation](http://adv-r.had.co.nz/Computing-on-the-language.html)

Die Funktionen `bquote()` und `eval()` aus dem R base Package helfen das Problem zu umgehen.

```{r, error = TRUE, eval = FALSE}
expect_floor_equal <- function(unit, time) {
  as_time <- function(x) as.POSIXct(x, tz = "UTC")
  eval(bquote(expect_equal(floor_date(base, .(unit)), as_time(.(time)))))
}

test_that("Beispiel X", {
  expect_floor_equal("year", "2008-01-01 00:00:00")
})
```

Diese Art des Refactorings kann sehr hilfreich sein. Der Testing Code wird dadurch besser lesbar und auch besser kognitiv überprüfbar, was das Vertrauen in den Testing Code stärkt.

```{r}
test_that("floor_date works for different units", {
  as_time <- function(x) as.POSIXct(x, tz = "UTC")
  expect_floor_equal <- function(unit, time) {
    eval(bquote(expect_equal(floor_date(base, .(unit)), as_time(.(time)))))
  }

  base <- as_time("2009-08-03 12:01:59.23")
  expect_floor_equal("second", "2009-08-03 12:01:59")
  expect_floor_equal("minute", "2009-08-03 12:01:00")
  expect_floor_equal("hour",   "2009-08-03 12:00:00")
  expect_floor_equal("day",    "2009-08-03 00:00:00")
  expect_floor_equal("week",   "2009-08-02 00:00:00")
  expect_floor_equal("month",  "2009-08-01 00:00:00")
  expect_floor_equal("year",   "2009-01-01 00:00:00")
})
```

## Tricks für CRAN   
Sobald ein R package auf CRAN deployed werden soll, müssen an den Tests eventuell einige Anpassungen gemacht werden. CRAN testet immer auf allen verfügbaren Plattformen, d.h. Windows, Mac, Linux und Solaris. Daher müssen plattformspezifische Tests möglichst vermieden werden.

Die folgenden Anhaltspunkte nennt [@Wickham2015]:  

1. Tests müssen auf CRAN relativ schnell laufen. Zielwert ist weniger als eine Minute. Tests, welche eine sehr lange benötigen können mit `skip_on_cran()` versehen werden. Damit werden Sie auf CRAN nicht ausgeführt.
1. Tests werden immer in Englisch ausgeführt ($LANGUAGE=EN$) und sind $C$ sortiert ($LC_COLLATE=C$). Dies minimiert Differenzen zwischen den Plattformen.
1. Gewisse Dinge können auf CRAN Maschinen anderes Verhalten aufweisen, als auf lokalen R Installationen. Beispielsweise können Laufzeiten stark variieren, da CRAN Maschinen stark ausgelastet sind. Auch Parallelisierungen können eventuell nicht funktionieren, da auf CRAN verschiedene Prozesse parallelisiert werden und dadurch für das Testing eines Packages möglicherweise nur ein Kern zur Verfügung steht. Auch die nummerische Präzision kann zwischen verschiedenen Plattformen variieren. Daher sollte eher `expect_equal()` statt `expect_identical()` verwendet werden.



## `testthat` in Skripts nutzen {#testthat_in_skripts}  
...

```{r}
library(testthat)

add <- function(x, y) {
  return(x + y)
}

test_that("add works", {
  expect_equal(add(2, 2), 4)
  expect_error(add("2", 2))
})
```






\newpage

# Nützliche Erweiterungen zu `testthat`    
`testthat` kann sinnvoll erweitert werden.  

## `devtools`  


## `assertthat`  
[@Wilson2014, Plan for mistake], 

## `covr`  
[@Hester2017]


\newpage

# Zusammenfassung des Vorgetragenen und Bewertung der Ergebnisse

\newpage

\listoffigures

# Literatur





