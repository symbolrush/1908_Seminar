---
title: | 
    | Seminar Programmiersysteme
    | T52: testthat - Testing in R
author: |
    | 
    | Adrian Stämpfli
    | Matr.Nr. 9529020
    | 
header-includes: \usepackage[ngerman]{babel} \usepackage{graphicx} \usepackage{float} \pagenumbering{roman} 
output:
  pdf_document:
    fig_caption: yes
    highlight: tango
    number_sections: yes
    toc: yes
date: | 
    |  
    | `r format(Sys.time(), '%B %d, %Y')`
documentclass: report
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
bibliography: bibliography.bib
preamble: |
  % Any extra latex you need in the preamble
abstract: |
  Dieser Seminarbeitrag stellt das `testthat` Package für automatisierte Testen von R packages vor. Einleitend wird eine kurze Einführung in R gegeben, sowie einige grundlegende Gedanken zum Testing allgemein, sowie zum Testing im Scientific Computing vorgestellt. Abgeschlossen wird der Beitrag von einen Gedanken zur Erweiterung von `testthat`, um weitere Konzepte des Testings in R integrieren zu können.
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

\newpage
\pagenumbering{arabic} 

# Einführung in R  
2 Seiten

## Was ist R?  
R ist eine Programmiersprache.[^11]  
[siehe @Peng2014, Abschnitt 3.1 - 3.8]  

..und [@Wickham2017, Abschnitt 1.3.2 und 1.4 - 1.6]

### Getting help and learning more  
[@Wickham2017, Abschnitt 1.6]



### RStudio  
[@Wickham2017, Abschnitt 1.4.2]

## Warum R?  
Nützlich für Scientific Computing und Data Sciences. [@Peng2014, Introduction]

[^11: https://www.r-project.org/]

\newpage

# Testing in R  

## Warum Testing?  
Testing ist essenziell, denn jede Software wird getestet. [@Hunt2000] schreiben dazu in Ihrem Buch "The Pragmatic Programmer": "Test Your Software, or Your Users Will". Also lieber selber die Bugs finden, als von seinen Kunden (oder Kollegen) darauf hingewiesen werden.  
Dennoch testen viele Programmierer nicht gerne. Testen wird als notwendiges Übel betrachtet, als Zusatzaufwand den es zu minimieren gilt. Häufig wird daher "nett" getestet: Kritische Stellen werden eher umschifft und nicht auf Herz und Nieren geprüft. [@Hunt2000, 237ff.]  
Im Scientific Computing ist die Situation dabei eher noch schlechter als in der traditionellen Softwareentwicklung. Dies hat mehrere Gründe. [@Wilson2014] schreiben, dass Wissenschaftler zwar mehr und mehr Zeit mit der Entwicklung von Software verbringen, jedoch meistens kaum fundierte Kenntnisse in Softwareentwicklung haben. Dabei ist Software für die heutige Wissenschaft so wichtig wie Reagenzgläser und Teleskope. In der Folge wird von denselben Wissenschaftler, welche teure Messgeräte einsetzen und diese regelmässig kalibrieren schlecht getesteter Code für die Auswertung der Messdten benutzt. Die Folge sind nicht selten falsche Ergebnisse. [@Wilson2014] geben eine ganze Reihe von Artikeln aus bekannten wissenschaftlichen Journals an, welche sich mit der Richtigstellung von falschen Untersuchungen aufgrund von Programmierfehlern beschäftigen. Testing ist also wichtig, wenn die Software welche getestet wird nicht von zahlenden Kunden genutzt wird.  
[@Hunt2000]
Tools, welche das (automatisierte) Testen unterstützen gibt es viele. Alleine in R gibt es mit `svUnit` und `rUnit` zwei Implementierungen des XUnit Frameworks.  
Warum also braucht es mit `testthat` nochmals ein neues Framework. [@Hunt2000] argumentieren in Ihrem Buch, dass Testing mehr ein kulturelles als ein technisches Problem sei. Die Kultur des Testens könne in ein Projekt eingebunden werden unabhängig von der Programmiersprache. [@Hunt2000, S.197]  
Genau hier setzt `testthat` an. Der Entwickler von Testthat Hadley Wickham, Chief Scientist bei RStudio und Autor mehrerer Bücher und der wichtigsten R packages der letzten Jahre [^21], schreibt hierzu, dass Software Testing wichtig sei, aber, dass es viele nicht tun, weil es frustrierend und langweilig ist. [@Wickham2011]  `testthat` versucht dies zu ändern, indem es sich besonders einfach in den Workflow von R Programmierern einbinden lässt, einfach zu lernen ist und gut skaliert. Insbesondere der schnell mögliche Einstieg in `testthat` versucht genau die "Kultur des Testens" zu katalysieren.  
Er argumentiert, dass automatisiertes Testen zwar etwas Zusatzaufwand bedeutet, sich jedoch in viererlei Hinsichten ausbezahlt:  

1. **Weniger Frustration**. Die Arbeit an der aktuell zu entwickelnden Software aufzugeben, um Bugs in (alter) Software zu suchen ist frustrierend. Gut getestete Software hat weniger Bugs, weniger Zeit geht mit der mühsamen Suche nach (alten) Bugs verloren.  
2. **Bessere Code Struktur**. Gut geschriebener Code ist einfacher zu testen. Häufig führt eine Kultur des Testens auch dazu, dass der Code umgeschrieben wird, bis er besser testbar ist. Dasselbe Argument führen auch [@Hunt2000] ins Feld.  
3. **Besserer Wiedereinstieg in die Arbeit nach einer (längeren) Pause**. Wenn eine Codeing-Sitzung damit abgeschlossen wird einen Test für das nächste zu entwickelnde Feature zu schreiben ist bei der Wiederaufnahme der Arbeit sofort klar, welcher Test einen Fehler wirft und welche Methode demzufolge als nächstes entwickelt werden sollte.  
4. **Mehr Vertrauen bei Änderungen am Code**. Wenn ein Entwickler weiss, dass jede Funktion gut getestet ist, lassen sich Änderungen mit viel besserem Gefühl umsetzen.  

Meine Erfahrung bei der Entwicklung von `sim911`, einer Sammlung von R packages für die Analyse und Simulation von Rettungsdiensteinsatzdaten, welche ich 2014 begonnen habe und in der Zwischenzeit von einem 4-köpfigen Team am IMS-FHS weiterentwickelt wird hat mich zusätzlich noch einen weiteren Punkt gelernt:  
5. **Insgesamt mehr Vertrauen in den (eigenen) Code**. Einige Teile von `sim911`[^22] sind viel besser getestet als andere. Das Vertrauen in diese Teile ist viel höher, als in die anderen. Wenn ich einem Kollegen helfe einen Bug in einem Projekt zu finden ist dieses Vertrauen essentiell. Bei den schlecht getesteten Teilen bin ich mir einige Jahre nach dem Entwickeln selber nicht mehr sicher, ob sie jetzt mit anderen Situationen umgehen können oder nicht. Dies führt dazu, dass immer mal wieder Zeit damit verloren geht, das Vertrauen in (alten) Code zurückzugewinnen. Häufig ist der Bug dann doch nicht im schlecht getesteten Code. Wäre der Code gut getestet, wäre dies jedoch von Beginn weg klar.     

[@Wickham2015] noch einarbeiten..

[^21: https://www.tidyverse.org/]
[^22: https://www.fhsg.ch/fhs.nsf/files/IMS_Rettungswesen_sim911Bericht/$FILE/1%20-%20sim911%20-%20Ein%20Simulator%20fu%CC%88r%20das%20Rettungswesen.pdf]

[@Wilson2014, Abschnitt "Plan for Mistake" und Einführung]
[@Hunt2000, 189ff., 237ff.]

## Testing Approaches in R  
In R existieren viele Testing Approaches. [@YihuiXie], Autor von `knitr` und weiteren R packages im Bereich "Publishing", nennt in seinem Blogbeitrag "Testing R Packages" drei formale Approaches und schlägt mit seinem package `testit` noch einen vierten vor. Zusätzlich ist sicherlich noch das informelle Testen von R Code in der Konsole zu nennen, laut [@Wickham2011] eine häufige Praxis in der R Community.  

Die 5 Testing Approaches sind demnach:  

1. Testing in der Konsole  
2. Testing mittels Textvergleich  
3. `RUnit` und `svUnit`  
4. `testit`  
5. `testthat`    


### Testing in der Konsole  
R ist eine Skriptsprache. Das heisst jede Zeile Code kann zu jedem Zeitpunkt ohne Compilieren ausgeführt werden. R Programmierer nutzen diese Eigenschaft typischerweise sehr intensiv: Bei der Entwicklung einer Funktion werden die einzelnen Bestandteile direkt in der Konsole getestet, korrigiert und danach im Skript platziert. Das heisst das Testing in der Konsole ist typischerweise die erste Variante, wie R Code getestet wird. [@Wickham2011] schreibt hierzu: "es ist nicht so, dass wir unseren Code nicht testen, aber wir speichern die Tests nicht, so dass wir sie automatisert wieder laufen lassen können."

### Testing mittels Textvergleichs  
Testing mittels Textvergleich ist ein älteres Testverfahren, welches in der R Community weit verbreitet ist. Die R Core packages beispielsweise werden auf diese Weise getestet.  
Dabei werden Testfälle unter `package/tests/` gespeichert. Mittels `R CMD BATCH foo-test.R` werden die Tests ausgeführt. Die Outputs aus dem Test werden dadurch im File `foo-test.Rout.save` gespeichert. Das Testing geschieht danach indem das File `foo-test.Rout`, generiert mit `R CMD check` mit dem vorab gespeicherten `foo-test.Rout.save` verglichen wird. Die Funktion `R CMD check` informiert den Tester automatisch, falls Differenzen bestehen. [@YihuiXie]

### `RUnit` und `svUnit`   
Mit `RUnit` und `svUnit` existieren zwei packages, welche das XUnit Framework in R implementieren. `RUnit` und `svUnit` sind ähnlich aufgebaut und orientieren sich an `jUnit`. Beide packages konnten sich in der R Community jedoch nicht durchsetzen. [@Wickham2011] und [@YihuiXie] schreiben, dass der Einarbeitungs- und Einrichtungsaufwand wohl bei beiden Packages zu gross sei.

### Testing mittels `testit`  
For me, I only want one thing for unit testing: I want the non-exported functions to be visible to me during testing; unit testing should have all “units” available, but R’s namespace has intentionally restricted the objects that are visible to the end users of a package, which is a Very Good Thing to end users. It is less convenient to the package author, since he/she will have to use the triple colon syntax such as foo:::hidden_fun() when testing the function hidden_fun().

I did not like the first approach (text comparison), and I did not want to learn or remember the new vocabulary of RUnit or testthat. There is only one function for the testing purpose in this package: assert().



### Testing mittels `testthat`  
`testthat` ist neuer als `RUnit` und `svUnit`. Der Autor Hadley Wickham ist einer der wichtigsten R-Entwickler der letzten Jahre, wenn nicht der wichtigste überhaupt.[^23]  

`testthat` hat sich in den letzten Jahren als de-facto Standard für das automatisierte Unit-testing in R etabliert.[^24] Im Folgenden wollen wir uns daher vertiefter mit `testthat` auseinandersetzen.


tests are expressed as expect_something() like a natural human language


[^23]: Laut https://www.rdocumentation.org ist Hadley Wickham der Topautor überhaupt. Seine packages werden durch die Community am meisten genutzt.
[^24]: Die Zunahme an packages mit formalen Tests ist quasi alleine auf testthat zurückzuführen: https://github.com/rstudio/webinars/blob/master/28-covr/covr-Rstudio_webinar.pdf


# Testing mit dem Package `testthat`  
[@Wickham2015], [@Wickham2011]


## Test Workflow   
Die Integration von `testthat` in ein bestehendes Package ist enorm einfach. Die Funktion `devtools::use_testthat` aus dem `devtools`[^31] Package erstellt die nötigen Ordner und Files. Dies sind:

1. Ein Ordner `tests/testthat` im Verzeichnis des aktuellen Packages.  
2. Ein File `tests/testthat.R`, welches alle Tests ausführt, wenn `R CMD check` aufgerufen wird. Dies ist wichtig für das Deployment eines Packages auf CRAN.  

Auch der weitere Workflow ist erdenklich einfach:  

1. Änderungen an Code oder Tests vornehmen.  
2. Das Package testen mit `devtools::test()` oder Ctrl/Cmd-Shift-T.  
3. Schritte 1 und 2 wiederholen, bis das Package fehlerfrei ist.  

`testthat` erstellt während dem Testen automatisch einen Statusreport in der Konsole oder in RStudio.

## Test Struktur   
`testthat` organisiert die Tests in einer hierarchischen Struktur. *expectations* werden zu *tests* zusammengefasst, mehrere *tests* werden in einem *context* zusammengefasst. Typischerweise besteht ein 1:1 Matching zwischen *context* und Datei. Dies ist aber nicht eine Notwendigkeit, eine Datei kann auch mehrere *contexts* enthalten.  

- *expectations*, also Erwartungen, sind die kleinste Einheit des Testens. Mit einer *expectation* wird das bei einer Berechnung erwartete Resultat beschrieben.  
- Ein *test* gruppiert mehrere *expectations*, um eine Eigenschaft einer Funktion zu testen. Im Falle von einfachen Funktionen kann ein *test* auch eine gesamte Funktion testen.  
- Der *context* gruppiert mehrere zusammengehörige Tests und gibt der Gruppe von Tests einen verständlichen Namen, welcher auch im Statusreport erscheint.  

## Expectations - Erwartungen  
Expectations sind die kleinste Einheit des Testens. Eine Expectation macht eine binäre Assertion darüber, ob das Resultat eines Funktionsaufrufs mit dem erwarteten Resultat übereinstimmt. Alle Expectations haben dieselbe Struktur:  

1. Die Funktionensnamen starten mit `expect_`
1. Sie haben zwei Argumente, das erste Argument ist das Resultat der zu testenden Funktion, das zweite das erwartete Resultat.
1. Eine Expectation wirft einen Fehler, wenn die beiden Resultate nicht übereinstimmen.

Das `testthat` Package umfasst mittlerweile an die 30 Expectations. Die wichtigsten daraus werden im Folgenden einzeln vorgestellt.  

> Eventuell umschreiben, anhand Dokumentation von testthat. Die Organisation dort ist: Test auf Gleichheit, Vergleichstests, etc...
https://cran.r-project.org/web/packages/testthat/testthat.pdf

### `expect_equal()` und `expect_identical()`  
afdad

### `expect_match()`  


### `expect_message()`, `expect_warning()` und `expect_error`  

### `expect_is()`

### Weitere Expectations  



## Tests  
Jeder Test sollte erstens einen informativen Namen haben und zweitens eine einzige Funktionalität testen. Der informative Namen beschreibt dabei die Idee des Tests. [@Wickham2015] schlägt vor, dass der Name des Tests den Satz "Test that..." vervollständigt. Diese Konvention erhöht die Lesbarkeit und hilft die Idee des Tests auch in Zukunft zu verstehen. Die Namensgebung entlangt dem Pattern "Test that..." ist ebenfalls eine schöne Anwendung des Axioms "Write programs for people, not computers", wie es u.A. [@Wilson2014] erwähnen. Dass ein Test nur eine einzige Funktionalität testen soll ist ebenfalls einsichtig: so ist direkt klar, welche Funktion einen Fehler verursacht. Tests werden in `testthat` mithilfe der Funktion `test_that()` geschrieben. `test_that()` nimmt zwei Argumente, das Argument `desc` enthält den Namen des Tests und das Argument `code` die Testanweisungen in geschleiften Klammern. Das folgende Beispiel zeigt diese Teststruktur. Das Beispiel stammt aus der Dokumentation von `testthat`.[^32]

```{r, eval = FALSE}
test_that("trigonometric functions match identities", {
  expect_equal(sin(pi / 4), 1 / sqrt(2))
  expect_equal(cos(pi / 4), 1 / sqrt(2))
  expect_equal(tan(pi / 4), 1)
})
```

Die Zuordnung der Expectations zu Tests ist in `testthat` nicht eingeschränkt. Ein Test kann 1:n Expectations enthalten. [@Wickham2015] schlägt vor, nicht zu viele Expectations einem Test zuzuordnen, so dass die Fehlersuche schnell vonstatten geht.

Jeder Test wird von R in einem eigenen Environment ausgeführt und ist in sich abgeschlossen. Alle zur Durchführung eines Tests notwendigen Daten müssen also in diesem Test generiert werden. Das folgende Beispiel zeigt diesen Sachverhalt:

```{r, eval = FALSE}
test_that("numbers are added correctly", {
  a <- 2
  b <- 1
  expect_equal(a + b, 3)
})

test_that("the result of an addition is still a numeric", {
  expect_is(a + b, "numeric")
})
```

Der erste Test überprüft, dass die Addition `a + b` das richtige Resultat zurückgibt, der zeite Test prüft die Klasse des Resultats. Dieser Code kann so jedoch nicht ausgeführt werden, da der zweite Test die beiden Variablen `a` und `b` nicht kennt, gibt R einen Fehler

```{r, eval = FALSE}
Fehler: Test failed: 'the result of an addition is still a numeric'
* Objekt 'a' nicht gefunden
```

zurück. Werden die beiden Variablen jedoch ausserhalb der Tests definiert, findet R sie im `Global Environment` und die beiden Tests laufen korrekt durch.  

```{r, eval = FALSE}
a <- 2
b <- 1

test_that("numbers are added correctly", {
  expect_equal(a + b, 3)
})

test_that("the result of an addition is still a numeric", {
  expect_is(a + b, "numeric")
})
```

`testthat` detektiert jedoch nicht, ob ein Test etwas an der R "Landschaft" ändert. Änderungen am Dateisystem, am Suchpfad (aufgrund von Calll der Funktionen `library()` oder `attach()`) und an den globalen Optionen (`options()` und `par()`) werden nicht erkannt.  

Wenn Tests diese Aktivitäten nutzen, muss der Anwender selber sicherstellen, dass keine unerwünschten Nebenwirkungen auftreten. Obwohl viele andere Testing Frameworks dies sicherstellen argumentiert [@Wickham2015], dass dies im Kontext von `testthat` nicht so wichtig sei. Wie das Beispiel oben zeigt ist es einerseits möglich Objekte, welche von mehreren Tests genutzt werden ausserhalb der Tests zu definieren. Für das Rückgängigmachen anderer Aktionen schlägt [@Wickham2015] die Nutzung regulärer R Funktionalität vor.

[^32]: https://www.rdocumentation.org/packages/testthat/versions/1.0.2/topics/test_that

## Kontexte und Dateien  
Der Kontext ist die oberste Organisationsstruktur innerhalb von `testthat`. Ein Kontext gruppiert eine Menge von Tests, welche verwandte Funktionalität testen. Ein Kontext wird durch den Call der Funktion `context("Name des Kontextes")` etwabliert. Die Namen der Kontexte erscheinen auch im Konsolenoutput beim Testen. 

```{r, eval = FALSE}
context("Adding numbers with R")

a <- 2
b <- 1

test_that("numbers are added correctly", {
  expect_equal(a + b, 3)
})

test_that("the result of an addition is still a numeric", {
  expect_is(a + b, "numeric")
})
```


Hadley Wickham schlägt in [@Wickham2015] vor, dass es eine 1:1 Zuordnung Kontext <> Datei gibt. In früheren Dokumenten [@Wickham2011] sowie in der Dokumentation der `context` Funktion ist diese Leitlinie noch weniger fix formuliert.

Aufgrund meiner eigenen Erfahrungen schlage ich den folgenden "Algorithmus" für die Organisation von Kontexten und Dateien vor:

1. Alle Tests sind in einer Datei.
1. Kontexte werden nach und nach eingeführt, sobald sich Inhaltliche Gruppierungen ergeben.
1. Sobald der Testcode einer Datei mehr als 100 Zeilen umfasst wird der längste Kontext dieser Datei in eine eigene Datei ausgelagert.

Dieses Vorgehen hat nach meinen Erfahrungen einen entscheidenden Vorteil gegenüber der von Beginn weg starren 1:1 Zuordnung. Zu Beginn einer Entwicklung ist das Naming der Funktionen und damit auch das des Testcodes (Kontexte und Tests) noch sehr volatil. Wenn hier von Beginn weg eine (zu) granulare Organisationsstruktur eingehalten wird, führt das Entweder zu häufigem Wechseln von Dateinamen und Kontexten, was ein unnötiger Aufwand ist, oder aber zu Beginn eingeführte (und häufig nicht optimale) Namensgebungen werden beibehalten, um Änderungen zu vermeiden. Die Nutzung der flexiblen Regeln nach obigem Algorithmus verhindert dies.

## Was testen?  
Nachdem wir wissen, *wie* mit `testthat` getestet werden kann, widmen wir uns nun der viel wichtigeren und nicht so eindeutig beantwortbaren Frage *was* (mit `testthat`) getestet werden soll. Im Folgenden möchte ich mich dieser "philososphischen" Frage anhand verschiedener Zitate nähern und durch meine eigene Einschätzungen abrunden.  

Starten möchte ich mit Martin Fowler[^33], einem der Gründerväter der agilen Softwareentwicklung:

> "Whenever you are tempted to type something into a print statement or a debugger expression, write it as a test instead."

Diese Guideline ist sicherlich ein guter Startpunkt. Die Frage ist jedoch, ob man so bei (zu) vielen Testfällen landet, welche künftige Änderungen am Code erschweren.

Hadley Wickham schreibt hierzu in [@Wickham2015]:

> "There is a fine balance to writing tests. Each test that you write makes your code less likely to change inadvertently; but it also can make it harder to change your code on purpose."

Hadley Wickham schreibt in seinem Buch auch, dass es schwierig sei generell gültige Aussagen zur Menge der notwendigen Tests zu geben. Wo also beginnen?

Programmierer im Umfeld von R bedeutet nutzen beim Entwickeln häufig die Interaktivität von R aus und testen sowohl einzelne Zeilen als auch ganze Funktionen interaktiv in der Konsole. Diese Testfälle direkt in `testthat` zu implementieren ist sicherlich ein guter Startpunkt und auch, wie Abschnitt [3.10 `testthat` in Skripts nutzen](#testthat_in_skripts) gezeigt wird, ohne grossen Aufwand möglich. Diese Art von Tests zu Automatisieren ist auch eine der Hauptmotivationen, welche Hadley Wickham in [@Wickham2011] darlegt:  

> "I'd perform many interactive tests to make sure the code worked, but I never had a system for retaining these tests and running them, again and again."

Ein weiterer enorm wichtiger Punkt spiegelt das folgende Zitat aus "The Pragmatic Programmer" wieder:

> "Find Bugs Once."

Sobald ein Tester einen Bug findet, sollte ein Testfall geschrieben werden, welcher den Bug abfängt. Falls derselbe Bug ein zweites Mal auftritt sollte er von `testthat` automatisch gefunden werden und nicht wieder vom Entwickler, Tester oder Anwendern. [@Hunt2000] Denselben Rat gibt auch Hadley Wickham in [@Wickham2011] und Wilson et al. geben in [@Wilson2014] den Satz:

> "Turn bugs into test cases."

als eine der vorgeschlagenen Best Practices an.

Im Umfeld des Scientific Programming ist zudem eine andere Guideline eine gute Entscheidungshilfe, wann Testfälle geschrieben werden sollen. Tests sollen sicherstellen, dass Funktionen dem Problemverständnis des Wissenschaftlers entsprechendes Verhalten aufweisen. Hierfür werden z.B. Tests geschrieben, welche Funktionsoutput mit vereinfachenden Rechnungen, mit Daten aus Experimenten oder mit Resultaten älterer, vertrauenswürdiger Software vergleichen. [@Wilson2014]

Wilson et al. schreiben hierzu:

> "Tests check to see whether the code matches the researcher's expectations of its behavior, which depends on the researcher's understanding of the problem at hand."

Eine weitere Dimension der Frage *was* man testen sollte wird in den folgenden beiden Zitaten angeschnitten.  

> "Avoid testing simple code that you're confident will work."

Dies schreibt Hadley Wickham in [@Wickham2015] und [@Hunt2000] schreiben:

> "Most developers hate testing. They tend to test gently, subconcsciously knowing where the code will break and avoiding the weak spots."

Die Menge der Testfälle garantiert eben noch keine gute Qualität der Testfälle.

Die genannten Zitate und Dimensionen der Frage nach dem *was* erlauben die Destillation der folgenden Guidelines beim Schreiben von Tests für die Nutzung von `testthat` im Rahmen des Scientific Computing:

1. **Teste (als Wissenschaftler) deine inhaltliche Erwartungshaltung.** Was ist dein Modell der Funktion, welche du am Entwickeln bist? Gibt es (vereinfachende) Testfälle (z.B. obere- oder untere Schranken einer Berechnung), welche du ohne aufwändige Testfälle angeben kannst? Gibt es obere oder untere Extremwerte, welche getestet werden können? Welche Anwendungsfälle hast du im Kopf und würdest du auf der Konsole interaktiv testen wollen? Schreibe diese als Tests nieder. Dieser Ansatz kann gut auch mit der "test-first" Philosophie kombiniert werden.[^34]
1. **Turn bugs into testcases.** Sobald ein Bug auftaucht, schreibe einen Testfall, welcher den Bug abfängt. Korrigiere den Bug danach.
1. **Sei Rücksichtslos.** Schreibe als nächstes den Test, welcher deiner Meinung nach die grösste Chance hat nicht erfolgreich durchzulaufen. Siehe hierzu auch [@Hunt2000, S.237ff.]. Die Befolgung dieser Maxime stellt ein Stück weit auch sicher, dass nicht (zu) viele Tests geschrieben werden, welche kaum einen Impact auf die Güte der Tests haben.

Aus einer Informatiksicht lassen sich zusätzlich die folgenden Guidelines nennen:

4. **Fokussiere beim Testen auf das externe Interface.** Wird nur das Interface getestet und nicht die Implementierung, lassen sich Änderungen an der Implementierung trotz Tests durchführen. Wenn auch die Implementierung getestet wird, müssen bei Änderungen am Code auch die Tests geändert werden. [@Wickham2015]
5. **Teste jedes Verhalten in genau einem Test.** Änderungen am Code ziehen so nur Änderungen an genau einem Test nach sich. [@Wickham2015]

Ungeklärt ist nun noch *mit was* getestet werden soll. Meine eigene Arbeit hat mich in den letzten Jahren gelernt, dass im Umfeld des Scientific Computing unbedingt mit synthethischen und echten Daten getestet werden muss. Die Arbeit mit synthetischen Daten ist sehr einfach möglich. Diese lassen sich meist sehr einfach genererien. Die erwarteten Resultate lassen sich mit synthetischen Daten dabei meist kognitiv nachvollziehen. Daher sind synthetische Daten enorm wertvoll um die inhaltliche Erwartungshaltung zu testen und können sehr schnell formuliert werden. Bei Tests mit echten Daten hingegen ist es viel schwieriger (oder nahezu unmöglich) das korrekte Resultat kognitiv nachzuvollziehen. Die Verifizierung von korrekten Testresultaten ist also enorm schwierig. Dagegen haben reale Daten häufig andere Eigenschaften, als die synthetischen Testdaten. Die Nutzung von zusätzlichen Tests mit realen Daten erlauben es, zu schauen, ob eine Implementierung auch mit diesen Eigenschaften umgehen kann. Falls sich korrekte Resultate auf echten Daten nicht kognitiv bestimmen lassen können diese dennoch wertvoll sein und z.B. gegenüber oberen- oder unteren Schranken getestet oder für Regressionstests verwendet werden. Ich formuliere daher noch die letzte Guideline:

6. **Nutze echte und synthetische Daten.**



[^33]: https://martinfowler.com/
[^34]: http://www.extremeprogramming.org/rules/testfirst.html

## Tests skippen   
Es gibt Situationen, in denen es sinnvoll sein kann einen Test zu überspringen, ihn zu skippen.  
Dies kann verschiedene Gründe haben:  
1. Ein Test greift auf eine externe API zu, welche momentan offline ist.
1. Ein Test greift auf eine Datei zu, welche momentan nicht verfügbar ist. (Beispielsweise, weil sie auf einem Rechner eines anderen Netzwerks liegt)
1. Tests greifen auf andere Systeme zu, welche momentan nicht zur Verfügung stehen.

In solchen Situationen kann es nützlich sein, einen Test zu überspringen. So gibt `testthat` keine Fehlermeldung zurück, sondern vermeldet einfach, dass ein oder mehrere Tests übersprungen wurden.

Zu diesem Zweck stellt `testthat` die Funktion `skip()` zur Verfügung. Statt einem Fehler zurückzugeben schreibt `skip()` ganz einfach ein $S$ in die Konsole.

```{r, eval = FALSE}
test_that("...", {
  skip("Grund für das Skippen des Tests.")
  ...
})
```

Das Beispiel oben nutzt `skip()` auf statische Art. Um den Test wieder laufen zu lassen, muss `skip()` gelöscht, bzw. auskommentiert werden. Häufig ist es eine bessere Idee `skip()` dynamisch zu nutzen. Die Bedingung unter welcher geskippt werden soll kann programmatisch abgefangen werden. Der Test wird, sobald beispielsweise eine API wieder zur Verfügung steht automatisch wieder ausgeführt. Das folgende Beispiel aus [@Wickham2015] zeigt diese Art der Nutzung.

```{r, eval = FALSE}
check_api <- function() {
  if (not_working()) {
    skip("API not available")
  }
}

test_that("foo api returns bar when given baz", {
  check_api()
  ...
})
```


## Schreiben eigener Expectations  
As you start to write more tests, you might notice duplication in your code. For example, the following code shows one test of the `floor_date()` function from `library(lubridate)`. There are seven expectations that check the results of rounding a date down to the nearest second, minute, hour, etc. There's a lot of duplication (which increases the chance of bugs), so we might want to extract common behaviour into a new function.

```{r}
library(lubridate)
test_that("floor_date works for different units", {
  base <- as.POSIXct("2009-08-03 12:01:59.23", tz = "UTC")

  expect_equal(floor_date(base, "second"), 
    as.POSIXct("2009-08-03 12:01:59", tz = "UTC"))
  expect_equal(floor_date(base, "minute"), 
    as.POSIXct("2009-08-03 12:01:00", tz = "UTC"))
  expect_equal(floor_date(base, "hour"),   
    as.POSIXct("2009-08-03 12:00:00", tz = "UTC"))
  expect_equal(floor_date(base, "day"),    
    as.POSIXct("2009-08-03 00:00:00", tz = "UTC"))
  expect_equal(floor_date(base, "week"),   
    as.POSIXct("2009-08-02 00:00:00", tz = "UTC"))
  expect_equal(floor_date(base, "month"),  
    as.POSIXct("2009-08-01 00:00:00", tz = "UTC"))
  expect_equal(floor_date(base, "year"),   
    as.POSIXct("2009-01-01 00:00:00", tz = "UTC"))
})
```

I'd start by defining a couple of helper functions to make each expectation more concise. That allows each test to fit on one line, so you can line up actual and expected values to make it easier to see the differences:

```{r}
test_that("floor_date works for different units", {
  base <- as.POSIXct("2009-08-03 12:01:59.23", tz = "UTC")
  floor_base <- function(unit) floor_date(base, unit)
  as_time <- function(x) as.POSIXct(x, tz = "UTC")

  expect_equal(floor_base("second"), as_time("2009-08-03 12:01:59"))
  expect_equal(floor_base("minute"), as_time("2009-08-03 12:01:00"))
  expect_equal(floor_base("hour"),   as_time("2009-08-03 12:00:00"))
  expect_equal(floor_base("day"),    as_time("2009-08-03 00:00:00"))
  expect_equal(floor_base("week"),   as_time("2009-08-02 00:00:00"))
  expect_equal(floor_base("month"),  as_time("2009-08-01 00:00:00"))
  expect_equal(floor_base("year"),   as_time("2009-01-01 00:00:00"))
})
```

We could go a step further and create a custom expectation function:

```{r}
base <- as.POSIXct("2009-08-03 12:01:59.23", tz = "UTC")

expect_floor_equal <- function(unit, time) {
  expect_equal(floor_date(base, unit), as.POSIXct(time, tz = "UTC"))
}
expect_floor_equal("year", "2009-01-01 00:00:00")
```

However, if the expectation fails this doesn't give very informative output:

```{r, error = TRUE, eval = FALSE}
expect_floor_equal("year", "2008-01-01 00:00:00")
```

Instead you can use a little [non-standard evaluation](http://adv-r.had.co.nz/Computing-on-the-language.html) to produce something more informative. The key is to use `bquote()` and `eval()`. In the `bquote()` call below, note the use of `.(x)` - the contents of `()` will be inserted into the call.

```{r, error = TRUE, eval = FALSE}
expect_floor_equal <- function(unit, time) {
  as_time <- function(x) as.POSIXct(x, tz = "UTC")
  eval(bquote(expect_equal(floor_date(base, .(unit)), as_time(.(time)))))
}
expect_floor_equal("year", "2008-01-01 00:00:00")
```

This sort of refactoring is often worthwhile because removing redundant code makes it easier to see what's changing. Readable tests give you more confidence that they're correct.

```{r}
test_that("floor_date works for different units", {
  as_time <- function(x) as.POSIXct(x, tz = "UTC")
  expect_floor_equal <- function(unit, time) {
    eval(bquote(expect_equal(floor_date(base, .(unit)), as_time(.(time)))))
  }

  base <- as_time("2009-08-03 12:01:59.23")
  expect_floor_equal("second", "2009-08-03 12:01:59")
  expect_floor_equal("minute", "2009-08-03 12:01:00")
  expect_floor_equal("hour",   "2009-08-03 12:00:00")
  expect_floor_equal("day",    "2009-08-03 00:00:00")
  expect_floor_equal("week",   "2009-08-02 00:00:00")
  expect_floor_equal("month",  "2009-08-01 00:00:00")
  expect_floor_equal("year",   "2009-01-01 00:00:00")
})
```

## Tricks für CRAN   
Sobald ein R package auf CRAN deployed werden soll, müssen an den Tests eventuell einige Anpassungen gemacht werden. CRAN testet immer auf allen verfügbaren Plattformen, d.h. Windows, Mac, Linux und Solaris. Daher müssen plattformspezifische Tests möglichst vermieden werden.

Die folgenden Anhaltspunkte nennt [@Wickham2015]:  

1. Tests müssen auf CRAN relativ schnell laufen. Zielwert ist weniger als eine Minute. Tests, welche eine sehr lange benötigen können mit `skip_on_cran()` versehen werden. Damit werden Sie auf CRAN nicht ausgeführt.
1. Tests werden immer in Englisch ausgeführt ($LANGUAGE=EN$) und sind $C$ sortiert ($LC_COLLATE=C$). Dies minimiert Differenzen zwischen den Plattformen.
1. Gewisse Dinge können auf CRAN Maschinen anderes Verhalten aufweisen, als auf lokalen R Installationen. Beispielsweise können Laufzeiten stark variieren, da CRAN Maschinen stark ausgelastet sind. Auch Parallelisierungen können eventuell nicht funktionieren, da auf CRAN verschiedene Prozesse parallelisiert werden und dadurch für das Testing eines Packages möglicherweise nur ein Kern zur Verfügung steht. Auch die nummerische Präzision kann zwischen verschiedenen Plattformen variieren. Daher sollte eher `expect_equal()` statt `expect_identical()` verwendet werden.



## `testthat` in Skripts nutzen {#testthat_in_skripts}  
...

```{r}
library(testthat)

add <- function(x, y) {
  return(x + y)
}

test_that("add works", {
  expect_equal(add(2, 2), 4)
  expect_error(add("2", 2))
})
```






\newpage

# Nützliche Erweiterungen zu `testthat`    
`testthat` kann sinnvoll erweitert werden.  

## `devtools`  


## `assertthat`  
[@Wilson2014, Plan for mistake], 

## `covr`  
[@Hester2017]


\newpage

# Zusammenfassung des Vorgetragenen und Bewertung der Ergebnisse

\newpage

\listoffigures

# Literatur





