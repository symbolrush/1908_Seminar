---
title: | 
    | Seminar Programmiersysteme
    | T52: testthat - Testing in R
author: |
    | 
    | Adrian Stämpfli
    | Matr.Nr. 9529020
    | 
header-includes: \usepackage[ngerman]{babel} \usepackage{graphicx} \usepackage{float} \pagenumbering{roman} 
output:
  pdf_document:
    fig_caption: yes
    highlight: tango
    number_sections: yes
    toc: yes
date: | 
    |  
    | `r format(Sys.time(), '%B %d, %Y')`
documentclass: report
geometry: margin=1in
fontfamily: mathpazo
fontsize: 11pt
bibliography: bibliography.bib
preamble: |
  % Any extra latex you need in the preamble
abstract: |
  Dieser Seminarbeitrag stellt das `testthat` Package für automatisierte Testen von R packages vor. Einleitend wird eine kurze Einführung in R gegeben, sowie einige grundlegende Gedanken zum Testing allgemein, sowie zum Testing im Scientific Computing vorgestellt. Abgeschlossen wird der Beitrag von einen Gedanken zur Erweiterung von `testthat`, um weitere Konzepte des Testings in R integrieren zu können.
---

```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.pos = 'H')
```

\newpage
\pagenumbering{arabic} 

# Einführung in R  
2 Seiten

## Was ist R?  
R ist eine Programmiersprache.[^11]  
[siehe @Peng2014, Abschnitt 3.1 - 3.8]  

..und [@Wickham2017, Abschnitt 1.3.2 und 1.4 - 1.6]

### Getting help and learning more  
[@Wickham2017, Abschnitt 1.6]



### RStudio  
[@Wickham2017, Abschnitt 1.4.2]

## Warum R?  
Nützlich für Scientific Computing und Data Sciences. [@Peng2014, Introduction]

[^11: https://www.r-project.org/]

\newpage

# Testing in R  

## Warum Testing?  
Testing ist essenziell, denn jede Software wird getestet. [@Hunt2000] schreiben dazu in Ihrem Buch "The Pragmatic Programmer": "Test Your Software, or Your Users Will". Also lieber selber die Bugs finden, als von seinen Kunden (oder Kollegen) darauf hingewiesen werden.  
Dennoch testen viele Programmierer nicht gerne. Testen wird als notwendiges Übel betrachtet, als Zusatzaufwand den es zu minimieren gilt. Häufig wird daher "nett" getestet: Kritische Stellen werden eher umschifft und nicht auf Herz und Nieren geprüft. [@Hunt2000, 237ff.]  
Im Scientific Computing ist die Situation dabei eher noch schlechter als in der traditionellen Softwareentwicklung. Dies hat mehrere Gründe. [@Wilson2014] schreiben, dass Wissenschaftler zwar mehr und mehr Zeit mit der Entwicklung von Software verbringen, jedoch meistens kaum fundierte Kenntnisse in Softwareentwicklung haben. Dabei ist Software für die heutige Wissenschaft so wichtig wie Reagenzgläser und Teleskope. In der Folge wird von denselben Wissenschaftler, welche teure Messgeräte einsetzen und diese regelmässig kalibrieren schlecht getesteter Code für die Auswertung der Messdten benutzt. Die Folge sind nicht selten falsche Ergebnisse. [@Wilson2014] geben eine ganze Reihe von Artikeln aus bekannten wissenschaftlichen Journals an, welche sich mit der Richtigstellung von falschen Untersuchungen aufgrund von Programmierfehlern beschäftigen. Testing ist also wichtig, wenn die Software welche getestet wird nicht von zahlenden Kunden genutzt wird.  
[@Hunt2000]
Tools, welche das (automatisierte) Testen unterstützen gibt es viele. Alleine in R gibt es mit `svUnit` und `rUnit` zwei Implementierungen des XUnit Frameworks.  
Warum also braucht es mit `testthat` nochmals ein neues Framework. [@Hunt2000] argumentieren in Ihrem Buch, dass Testing mehr ein kulturelles als ein technisches Problem sei. Die Kultur des Testens könne in ein Projekt eingebunden werden unabhängig von der Programmiersprache. [@Hunt2000, S.197]  
Genau hier setzt `testthat` an. Der Entwickler von Testthat Hadley Wickham, Chief Scientist bei RStudio und Autor mehrerer Bücher und der wichtigsten R packages der letzten Jahre [^21], schreibt hierzu, dass Software Testing wichtig sei, aber, dass es viele nicht tun, weil es frustrierend und langweilig ist. [@Wickham2011]  `testthat` versucht dies zu ändern, indem es sich besonders einfach in den Workflow von R Programmierern einbinden lässt, einfach zu lernen ist und gut skaliert. Insbesondere der schnell mögliche Einstieg in `testthat` versucht genau die "Kultur des Testens" zu katalysieren.  
Er argumentiert, dass automatisiertes Testen zwar etwas Zusatzaufwand bedeutet, sich jedoch in viererlei Hinsichten ausbezahlt:  

1. **Weniger Frustration**. Die Arbeit an der aktuell zu entwickelnden Software aufzugeben, um Bugs in (alter) Software zu suchen ist frustrierend. Gut getestete Software hat weniger Bugs, weniger Zeit geht mit der mühsamen Suche nach (alten) Bugs verloren.  
2. **Bessere Code Struktur**. Gut geschriebener Code ist einfacher zu testen. Häufig führt eine Kultur des Testens auch dazu, dass der Code umgeschrieben wird, bis er besser testbar ist. Dasselbe Argument führen auch [@Hunt2000] ins Feld.  
3. **Besserer Wiedereinstieg in die Arbeit nach einer (längeren) Pause**. Wenn eine Codeing-Sitzung damit abgeschlossen wird einen Test für das nächste zu entwickelnde Feature zu schreiben ist bei der Wiederaufnahme der Arbeit sofort klar, welcher Test einen Fehler wirft und welche Methode demzufolge als nächstes entwickelt werden sollte.  
4. **Mehr Vertrauen bei Änderungen am Code**. Wenn ein Entwickler weiss, dass jede Funktion gut getestet ist, lassen sich Änderungen mit viel besserem Gefühl umsetzen.  

Meine Erfahrung bei der Entwicklung von `sim911`, einer Sammlung von R packages für die Analyse und Simulation von Rettungsdiensteinsatzdaten, welche ich 2014 begonnen habe und in der Zwischenzeit von einem 4-köpfigen Team am IMS-FHS weiterentwickelt wird hat mich zusätzlich noch einen weiteren Punkt gelernt:  
5. **Insgesamt mehr Vertrauen in den (eigenen) Code**. Einige Teile von `sim911`[^22] sind viel besser getestet als andere. Das Vertrauen in diese Teile ist viel höher, als in die anderen. Wenn ich einem Kollegen helfe einen Bug in einem Projekt zu finden ist dieses Vertrauen essentiell. Bei den schlecht getesteten Teilen bin ich mir einige Jahre nach dem Entwickeln selber nicht mehr sicher, ob sie jetzt mit anderen Situationen umgehen können oder nicht. Dies führt dazu, dass immer mal wieder Zeit damit verloren geht, das Vertrauen in (alten) Code zurückzugewinnen. Häufig ist der Bug dann doch nicht im schlecht getesteten Code. Wäre der Code gut getestet, wäre dies jedoch von Beginn weg klar.     

[@Wickham2015] noch einarbeiten..

[^21: https://www.tidyverse.org/]
[^22: https://www.fhsg.ch/fhs.nsf/files/IMS_Rettungswesen_sim911Bericht/$FILE/1%20-%20sim911%20-%20Ein%20Simulator%20fu%CC%88r%20das%20Rettungswesen.pdf]

[@Wilson2014, Abschnitt "Plan for Mistake" und Einführung]
[@Hunt2000, 189ff., 237ff.]

## Testing Approaches in R  
In R existieren viele Testing Approaches. [@YihuiXie], Autor von `knitr` und weiteren R packages im Bereich "Publishing", nennt in seinem Blogbeitrag "Testing R Packages" drei formale Approaches und schlägt mit seinem package `testit` noch einen vierten vor. Zusätzlich ist sicherlich noch das informelle Testen von R Code in der Konsole zu nennen, laut [@Wickham2011] eine häufige Praxis in der R Community.  

Die 5 Testing Approaches sind demnach:  

1. Testing in der Konsole  
2. Testing mittels Textvergleich  
3. `RUnit` und `svUnit`  
4. `testit`  
5. `testthat`    


### Testing in der Konsole  
R ist eine Skriptsprache. Das heisst jede Zeile Code kann zu jedem Zeitpunkt ohne Compilieren ausgeführt werden. R Programmierer nutzen diese Eigenschaft typischerweise sehr intensiv: Bei der Entwicklung einer Funktion werden die einzelnen Bestandteile direkt in der Konsole getestet, korrigiert und danach im Skript platziert. Das heisst das Testing in der Konsole ist typischerweise die erste Variante, wie R Code getestet wird. [@Wickham2011] schreibt hierzu: "es ist nicht so, dass wir unseren Code nicht testen, aber wir speichern die Tests nicht, so dass wir sie automatisert wieder laufen lassen können."

### Testing mittels Textvergleichs  
Testing mittels Textvergleich ist ein älteres Testverfahren, welches in der R Community weit verbreitet ist. Die R Core packages beispielsweise werden auf diese Weise getestet.  
Dabei werden Testfälle unter `package/tests/` gespeichert. Mittels `R CMD BATCH foo-test.R` werden die Tests ausgeführt. Die Outputs aus dem Test werden dadurch im File `foo-test.Rout.save` gespeichert. Das Testing geschieht danach indem das File `foo-test.Rout`, generiert mit `R CMD check` mit dem vorab gespeicherten `foo-test.Rout.save` verglichen wird. Die Funktion `R CMD check` informiert den Tester automatisch, falls Differenzen bestehen. [@YihuiXie]

### `RUnit` und `svUnit`   
Mit `RUnit` und `svUnit` existieren zwei packages, welche das XUnit Framework in R implementieren. `RUnit` und `svUnit` sind ähnlich aufgebaut und orientieren sich an `jUnit`. Beide packages konnten sich in der R Community jedoch nicht durchsetzen. [@Wickham2011] und [@YihuiXie] schreiben, dass der Einarbeitungs- und Einrichtungsaufwand wohl bei beiden Packages zu gross sei.

### Testing mittels `testit`  
For me, I only want one thing for unit testing: I want the non-exported functions to be visible to me during testing; unit testing should have all “units” available, but R’s namespace has intentionally restricted the objects that are visible to the end users of a package, which is a Very Good Thing to end users. It is less convenient to the package author, since he/she will have to use the triple colon syntax such as foo:::hidden_fun() when testing the function hidden_fun().

I did not like the first approach (text comparison), and I did not want to learn or remember the new vocabulary of RUnit or testthat. There is only one function for the testing purpose in this package: assert().



### Testing mittels `testthat`  
`testthat` ist neuer als `RUnit` und `svUnit`. Der Autor Hadley Wickham ist einer der wichtigsten R-Entwickler der letzten Jahre, wenn nicht der wichtigste überhaupt.[^23]  

`testthat` hat sich in den letzten Jahren als de-facto Standard für das automatisierte Unit-testing in R etabliert.[^24] Im Folgenden wollen wir uns daher vertiefter mit `testthat` auseinandersetzen.


tests are expressed as expect_something() like a natural human language


[^23]: Laut https://www.rdocumentation.org ist Hadley Wickham der Topautor überhaupt. Seine packages werden durch die Community am meisten genutzt.
[^24]: Die Zunahme an packages mit formalen Tests ist quasi alleine auf testthat zurückzuführen: https://github.com/rstudio/webinars/blob/master/28-covr/covr-Rstudio_webinar.pdf


# `testthat`  
[@Wickham2015], [@Wickham2011]


## Test Workflow   
Die Integration von `testthat` in ein bestehendes Package ist enorm einfach. Die Funktion `devtools::use_testthat` aus dem `devtools`[^31] Package erstellt die nötigen Ordner und Files. Dies sind:

1. Ein Ordner `tests/testthat` im Verzeichnis des aktuellen Packages.  
2. Ein File `tests/testthat.R`, welches alle Tests ausführt, wenn `R CMD check` aufgerufen wird. Dies ist wichtig für das Deployment eines Packages auf CRAN.  

Auch der weitere Workflow ist erdenklich einfach:  

1. Änderungen an Code oder Tests vornehmen.  
2. Das Package testen mit `devtools::test()` oder Ctrl/Cmd-Shift-T.  
3. Schritte 1 und 2 wiederholen, bis das Package fehlerfrei ist.  

`testthat` erstellt während dem Testen automatisch einen Statusreport in der Konsole oder in RStudio.

## Test Struktur   
`testthat` organisiert die Tests in einer hierarchischen Struktur. *expectations* werden zu *tests* zusammengefasst, mehrere *tests* werden in einem *context* zusammengefasst. Typischerweise besteht ein 1:1 Matching zwischen *context* und Datei. Dies ist aber nicht eine Notwendigkeit, eine Datei kann auch mehrere *contexts* enthalten.  

- *expectations*, also Erwartungen, sind die kleinste Einheit des Testens. Mit einer *expectation* wird das bei einer Berechnung erwartete Resultat beschrieben.  
- Ein *test* gruppiert mehrere *expectations*, um eine Eigenschaft einer Funktion zu testen. Im Falle von einfachen Funktionen kann ein *test* auch eine gesamte Funktion testen.  
- Der *context* gruppiert mehrere zusammengehörige Tests und gibt der Gruppe von Tests einen verständlichen Namen, welcher auch im Statusreport erscheint.  

## Expectations - Erwartungen  
Expectations sind die kleinste Einheit des Testens. Eine Expectation macht eine binäre Assertion darüber, ob das Resultat eines Funktionsaufrufs mit dem erwarteten Resultat übereinstimmt. Alle Expectations haben dieselbe Struktur:  

1. Die Funktionensnamen starten mit `expect_`
1. Sie haben zwei Argumente, das erste Argument ist das Resultat der zu testenden Funktion, das zweite das erwartete Resultat.
1. Eine Expectation wirft einen Fehler, wenn die beiden Resultate nicht übereinstimmen.

Das `testthat` Package umfasst mittlerweile an die 30 Expectations. Die wichtigsten daraus werden im Folgenden einzeln vorgestellt.  

> Eventuell umschreiben, anhand Dokumentation von testthat. Die Organisation dort ist: Test auf Gleichheit, Vergleichstests, etc...
https://cran.r-project.org/web/packages/testthat/testthat.pdf

### `expect_equal()` und `expect_identical()`  
afdad

### `expect_match()`  


### `expect_message()`, `expect_warning()` und `expect_error`  

### `expect_is()`

### Weitere Expectations  



## Tests schreiben  

## Was testen?  

## Tests skippen  

## Schreiben eigener Expectations  

## Tricks für CRAN  

## `testthat` in Skripts nutzen  


```{r}
library(testthat)

add <- function(x, y) {
  return(x + y)
}

test_that("add works", {
  expect_equal(add(2, 2), 4)
  expect_error(add("2", 2))
})
```






\newpage

# Nützliche Erweiterungen zu `testthat`    
`testthat` kann sinnvoll erweitert werden.  

## `devtools`  


## `assertthat`  
[@Wilson2014, Plan for mistake], 

## `covr`  
[@Hester2017]


\newpage

# Zusammenfassung des Vorgetragenen und Bewertung der Ergebnisse

\newpage

\listoffigures

# Literatur





